[{"page_number": 1, "sentences": "arxiv240206196v2 cscl 20 feb 2024\n\nlarge language model\n\n survey\n\nshervin minace tomas mikolov narjes nikzad meysam chenaghlu\nrichard socher xavier amateiain janfeng gao\n\naburact\u2014large language models llm  drawn\ntot  attention se   stone performance  wie\n\not ata langage take nce theres  chat pt\nunderstanding  peneration acquired hy waning nef\n\u2018models parameters  musve amounts  teal data x predicted\n\u2018cali al te roar arn  lm le ry\n\n  survey popular datasets prepared  llm trainin\nfietuning  ealutin evew widely used llm evaluation\n\nsad compare  performance  several popular lems\nfina set  repraatatve bencaris finally  conde\nite fap  cing open ans  fie earh\n\n1 intmopucrion\n\nlanguage modeling  longstanding researc tpi dat\ning back tothe 1950s  shannon application  laf\n\u2018 theory 0 human language measured  well\n\u2018snple gram language models predict ce compress natural\ntanguage text 3 sine  stascal language modeling\n\u2018ecame fundamental  many natural lnguage understanding\n generation asks raging tom speech fecogaiton mae\nhie tanslton  information retietl 4 j 6\n\n\u2018 recent advances  trasforenbaeed lage language\n\u2018models lems pretaned  wesel tex compo sani\nican extended  espabiitis  language mods llas\n example open ls chatgpt  gpt ean  used ot\n tor natural language procesing  aso ax gener task\nvere  power microsofts copilot ystems  instance\nfan follow human insructons  complex new tsks pete\ntowming multstep reasoning  needed lems  thus\n\u2018evoming tbe baie bulling block   development \nseveral purpose al agents  aii general itlignce\naah fel  llms  moving fast new findings\nmodels  technaes  published   mater  ants\n weeks 7h 8 9 10  al researchers  pact\n\u2018ones often n challenging  figure   best recipes\n buld llmcpowered al systems   tasks paper\nies  timely survey   recent advances  llm hope thi survey wil prove  vluable  sccessble resource\n students escatehos  developers llms  largescale  nine statistical language mod\n\u00abfs bse  etal networks rvent ses  elm \nsn accummtion  decades  research  development \ntanguage models    eategorzed  four waves\n\n  diferent starting points  velocity satel lan\niquage models neural language model pretraned language\n\u2018dels nl len\n\nstatistical language models slms view text  sequence\n\u2018 words  estimate  prababiity  text  prac\nfof  werd probate dominating form  slms\nse\u2019 murkoy chain models known   ngram models\n\u2018hich compute  probability   word contioned  \ntediate proceeding n 1 wards since word probabiies\nae estimated usiag word  gram counts collected fom\ntext compra  model needs  deal  data sparsity le\n\u2018signing zero probabilities  unseen words  bras \nsing simoorhing probability mass   made\n reserved  unseen grams 2 ngram models \n\u2018widely used  many nlp sjstems howeter  models\nar incomplete  thi  cannot fully capture  diversity\n variably  natural language due  data sparsity\n\narly neural language moses nlms 13 1415 6\neal wit data sparsity  mapping woes 1 lowdimensional\ncontinuous vectors embedding veco  predict  next\n\u2018word based   agrezation   embedding vectors \n proceeding words sine neal networks  embedding\n\u2018vectors learned  nlm define  hidden space  \n\u201cmantic smilrty fstween vector canbe realy computed\nfv  distance  opens  door  computing semantic\nsarit   two inputs regardless bei ors eg queries\n\u2018documents  search 17 18 sentences  diferent\ntanguages  machine tansation 9 20  modaiis eg\nimage nd text  iage eaptoning 21 2 arly nlm \ntadkspecie models  tt   tained  task speie\n\u2018data   eared hidden space  tskespecie\n\nprearained language models plm unlike early nlm\nae taskeagnste ths generality also extends tothe leaued\nfiden embedding space  ning  inference  plm\nfoloms  preradning aed finetuning pra  lan\n\u2018euage modele  recuent neural networks 23  tune\ntowers 24 25 26 ae pe tained  webscale unlabeled\ntext corpora  general tas   word prediction  \nfnetuned  specie tasks wsing small amounts  abel\ntaskspecfic dit recent surveys  plms inl 8 27\npsi\n\nlage language models llms mainly refer 10\nseansformerbased neuf language models  tha conn\ntens  hundreds  bilions  parameters  ae pre\ntained  massive text dat   palm si llama\n32  gpt 33  summarized  table ml compared\n\npons se  oe se pce mo f150"}, {"page_number": 2, "sentences": "fig 1 llm capabiiis\n\n plms llms  ot  much larger  model size \nalo exhibit stangertanguage understanding  goneation\nsblies   important emergent abilities  \n\u2018ot present  smallerscale tanguage models usted\n figl emergent abies inctude 1 context\ntearing  llms earn anew usk  small se\nfof examples presented inthe prompt  inference time 2\nintact flowing llms ter insiruton tang\n follow  snsrctons  new types  tasks without\nsing exphictexampes  3 mule reasoning \nlem  solse complex usk  breaking dow  task\nino inermediate reasoning sepa demonstrated  \nchaiothought prompt 54 lems  also  aygmened\n using extemal knowledge  tots 38 36 30 \n cam effectively interact  uses  eassoament 37\nsand continually improve iuet using feedback dita collected\nfdvough interactions eg via feinfrcement tearing \njnumaa feedback lhp \u2018 advanced wage  augmentation techniques\nllm   deployed  called  agent arc emies\n seme thei envionment make decisions nd take ations\nprevious esearch hs ocuned  developing agents  specie\ntasks  domains emergent abies demonstrated \nlems mate  possible  bald generalparpowe ai agen\n\u2018ced  lems llm  wine pace responses\n\u2018nsatie wetings al aents ned  take actions  interact ith\naynamic enironment \u2018hereto llmbased agents often\n\u2018eed  augment lems coe obtain updated information\n extemal knowiedge tases verify whether ystem action\njoduces  expected resull  ope   things \nfot go  expected te discuss  detail llmased\npeas ia seton iv\n\n  rest ofthis pape seton il presents  overview \nstale ofthe   llms focusing oa  llm families gpt llama  palm   representative medls section\nim discusses  llms  uit section iv discusses \n\nllms  used  augmented  realworld applications\nsections v  vi review poplar datas  bepchinarks \ncalating llms  summarize  reported lem evalation\nrests finally section vii conclaes  paper  \nfizng  challenges  fate researeh dtetons 1h laroe laxcuace movers\n\n  section  sat  review  early prerined\neur language models     bas  lms \n focus oar dicusson  tee families  llms gpt\nlama  palm table  provides  overview   \n medels  thei characteris early pretrined neural language models\n\nlanguage modeling using neural neorks  poner\n 38 39 40 bengio ea 13 developed one ofthe st\n\u2018sour language models nlms  fe comparable  nram\n\u2018models 14 successfully applied nlm  machine\n\u2018tation eleae  rnnlm  open source nlm\noolkit  mikolo 41 42 helped signicanly popularize\nnems aftersards nlmis based  rcurent acral networks\nrnno  thee variants suchas log shorter memory\nustm 19  gated recurrent unit gru 20  widely\n\u2018tse  many natural language applications including machine\n\u2018tnslaton text generation apd ext classification 43\n\n\u2018  invention ofthe transformer architecture 45\nmarks another milestone inthe development  num applying selfateaion  compute  parle fe every word\nina sentence  document  afenion score\u201d  made \ninfluence  sord   anther transformers  \nrich  parallelization  rnns  makes  possible\n0 elfen pretuin vey hig langnage models  large\nmounts  data  gpus pretrained language models\nplm canbe finetuned foe many downsteam tasks"}, {"page_number": 3, "sentences": "ff\n\nfig 2  paper sirvtare\n\n group early popular tansfrmerbased plms based \n neural artitseares int thre main estore encoder\n\u2018 decoderony  encoder decoder mals comprehen\nsve surveys  early plms  provided  3 28\n\n1 brcoderonty plms ashe name suggests  encode\n models aly consis ofan encoder network models\nfe originally developed  language understanding sks\n\u2018ich etext clanication   models need  predict \nlas abel   inp ext representative encoderonlymod\nfle ince bert  ie variants eg robert albert\ndeberts xlm xlnet unilm fo  desert blow\n\nbert birectonal encoder representations  trans\nformers 28 ts one ofthe  wey sed encoder nly\ntongtige mosis bert consis  three modes 1 \n\u2018bedding mode  convers inp text  8 seq\nff embedding veciors 2 stack  transformer encoders\n convent embedding vectors  contextual represeatation\nnectar  3 fully conpected layer thi comer \nrepresentation vectors tthe final layer 0 one hot veto\nbert  preained ues two objectives masked language\n\u2018modeling milm ad next sentene rection  pre tned\nbert model   finetuned  adding  clanier ler\n many language understanding tasks ranging fom tet"}, {"page_number": 4, "sentences": "\u2018table  highlevel overview  popular language models\n\nclassication question answering 10 language inference fighlevel overview  bert framework  shown  ig 3a\nbert signiicanly improved slate ofthe atom  wide ange\n language understanding tasks    pblishe  \ncommunity  inspied  develop many similar encoderoly\ntanguage models based  bert roberts 25 significantly improves  robustness \nbert using \u00abset  model design choices  waning sate\nes   moving afew key hyperparameter removing\n nextsenencepretaining objective  training  much\n\nlarger minibatces  euming rates albert 45 ses 160\npafumeterseduction techniques  wer memory consumption\n increase  taining speed  bert 1 spliting \nembedding matrix ino two smaller matces  2 using\nfepeating yer split among groups deberta decoding\n\u2018hanced bert  dsenangled attention 2 improves \nbert  roberta models using two novel techniques fists  dsentangled attention mochansm   word\n represented using two vectors  encode  content abd\npositon respectively te tention weights amang words"}, {"page_number": 5, "sentences": "fig 3 overall presraining  finetuning procedures \nbert couresy 0 24\n\nae computed using disetangled matrices  thst contents \n\u2018elas positions respectively second  enanced mask de\n\u2018oder nuked  incorporate absolte potion   decoding\ntaper wo predict  masked tokens  model pretaining 19\nakon novel viral versal waning method  use \ntne taming io iprove modele generalization electra 36\n\u2018ses  pes prestesning ask kon paced token detection\nrtd whichis empirically proven tobe  sampleetcient\nhap mlmc instead  musing  inpu red corrupts \n\u2018replacing  tokens  plausbeateatives sampled fom\n2 small gencratoe network instead  taining 2 model\n\u2018hat predicts  orginal identities   coerpted tokens 2\nuscrminative model  wained  peedict wheter token\n comapeed input  replaced   generated sample  \nrtd fs move sampleefcient tian mlm   former\njs detned overall put okens rather   te small subset\n\u2018 masked ou  hlustated  ig 4\n\nfig 4  comparison  replaced token detection \n\u2018masked language medcing courtesy  46\n\nxlms 47 extended bert  erosingusl language\n\u2018models wsing two metho 1  unsepervised method thst\nnly eles  monolingual data nd 2 supervised metho\nhat leverages parle data ith ane crosengual anguage\n\u2018model objective av ilusrated  fig 8 xlm  btaned\nsate heat resis  crosslingualclasiiation usuper\n\u2018ised  supervised machine watson thee hey \nproposed \u2018  also encoder language models tat leverage\n advantages  autoregressive decoder models  model\ntaining  inference examples  xlnet  unilm kxlnet 48  based  transformer xl preained using \nssneralized autoregressive method  ehables learning bi\n\u2018ectonal contexts  maximizing  expected likelihood \n\nwi\na8\n\n\n\nwine\n\nfig 5 cros\n\u2018objectives simular  bert  wth continuous streams\nfof text  opposed io sentence pais tlm objective\nfextends mlm  pits  parallel sentences predict 2\n\u2018asked english woe  mee  tend    english\nsentence ants french tatslaton  seacouragd  alg\nenglish  french representations coates  0\n\n permuations   factorization oder unilm uneed\n\u2018retained language model 49  preained using hee\n\u2018ypes  language moveling sks unidrectona bidirectional\n sequeneetosequence prediction achieved \n\u2018mploying sired transformer network  wing specie\nselfattention masks fo control  context  prediction \nconditioned ons lutte  fig 6 pretained model\n  finetuned tor  natural anguage understanding \neneration asks fig 6 overview  unfed lm presraining moet\npafumeters  shared across tbe lm abjectives ie biiec\n\u2018 lm unidirectional lm sequeeetosequeace ln\ncouey  19\n\n2 decnder plms two    widely used\ndecoderonly plms  gpfi  gpt2 developed \nopenal  models lay  foundation   powertal\nlems subsequently ie gps  gpt\n\ngpe 50 demonstrates forthe fst ime tht go\nperformance ove  wide range  natural langage asks  \nsbaained  generative pretraning gpt   decoderoly\n\u201ctransformer model  adverse corpus  unlabeled ext  \nselfsupervsed tearing fashion les next worden pedic"}, {"page_number": 6, "sentences": "foo followed  discriminative sinetuning   specific\n\u2018domsteam tak vith much fewer samples  ilusrated \nfig 7 gpt paves  way  subsequent gpt models cach version improving upon  architecture abd achieving\n\u2018ter performance  various langage task\n\n\u2014\u2014\u2014\u2014\u2014\n\n\nfig 7 highve overview  gpt peering nd haetwing\nsteps courtesy  openal\n\ngpt2 51 shows  language models ave able 10 learn\n perform specie natural language asks without  explicit\nsupervision  tained  lage webtext dataset consisting\n milins  webpages gpt model follows themes\n\u2018designe  gpt   modifications layer normale\njaaion  moved tothe input ofeach sublock additonal\nlayer narmlivaton  dd   inal ciatenton block\ninalizaion  modiied  account forthe accumulation \n\u2018 residual pah  scaling te weights  tesidul ayes\n\u2018vocabulary size  expanded  025  context size \nincreased  512  1024 tokens\n\n3 encoder decoder plm  32 rafe  shows \nalmost  nlp tasks   eat  4 sequence sequence\n\u2018eneraton sk thus  encoder decoder language model \u2018design  unied node  tht   perform  natal\ntanguage understanding  generation tasks representative\ncncialerdscoder plms  wil review   ts mits\n\u2018mass bart\n\n17 52 sa texetotet transfer transformer t3 model\n\u2018whet transfer lesming  effectively exploited  nlp \nintcton  \u00abune framework    nlp tasks \ncastas  textoteat generation ask 75 83 sa mulling\n\u2018arian  ts whichis petained  new common crave\n\u2018bse dataset consisting  texts  tol languages\n\nmass masked sequence 1 sequence prestaining 4\nopts  encodersesrer framework  reconstract sen\nence fragment given  remaining pat   sentence \nfocoder taken  sentence  randomly tasked agent\nseveral consecutive wens  input andthe decoder pedits\n masked tragment ths way mass jin wains \nencoder  dacoder fr language embedding  peneraton\nfespectvely\n\nbart 85 uses  standard sequencesequence tra\n\u2018 model architecture pre sned  comrepting txt \nsn utrary nosing hncton   leaming  reconsact\n orignal text b large language model families\n\nlarge language models llms mainly refer 16\nteanstrmerised plm  contain ene  hundreds\n\u2018ot ilions  parameters compared  plme reviewed ane\nllms    much ager model siz bat albo eat\nsronger language understanding  generation  emergent\nhes tht ate rot prevent  smaller seale models toons  review three llc families opt llama \npalm 3 stated  fig 8\n\n1  gpt family generative pretrsned transform\ncrs gpt   family  decaderonly transormer hse\ntanguage mics developed  openal aly eon\n\u2018int  gpe gpt2 gpt nsiracgpt chagpt opts\ncodex  webgpt although early gpt models sich a6\ngpp  gpt2 ae opensource recent modes sch 3\n\u2018gpt gpt4  cmesource     secesed\nvia apis gpt1  gpt2 models  beet dscssed\n caly plm suction sat  gpt gpt 56  pretrained autoregressive language model\nswith 175 hilion parameters gptes widely considered 9s\n fist llm  tht  ot   mac tage  previo\npms bot also   fis time demonsrates emergent\nilies    observed  previous sealer plm gpt\n3 shows  emerge ay  ncconext lesring \n\u2018means gpt canbe applied   downsteam tasks without\nny gradient updates  inetaning wit tanks   shot\n\u2018etaontation speed purely vi tent interaction  \n\u2018model gpt achieved srong perfomance  many nlp\neek includng ransation queionamwering   chore\nasks  well  several ones  rule oathny reasoning\nff demain adaptation   unscrambling words using 2\n\u2018novel word   sentence 3digitantumetc fig 9 plots \n\u2018evfonmance  gpt3 function ofthe nue  examples\n inscomext promps\n\nccodex sti released  openal  march 2003  3\ngeneral purpose programming mu thal  pare natal\ntanguage andgeneste coe  response codex  2 de\nscendant  gpt fnestuned  programing aplication\nfon cade corpora collected  gila codex powers\nmicrosot\u2019 gtfeb cop\n\nwedgpt 58  anther descendant  gpt finetuned 16\nanswer openndd questions using fexbased web browse\nfacing users  search  navigate  eb speci\nwedgpt  trained  thee steps fist   webgpt\n lear  mimic himan browsing behivors \u00absing aman\n\u2018demonstration dit reward fnction  learned\npredict human preferences finally weogpt  reine 0\nsptimize  reward tunstion via reinforcement lene ant\n\u2018ection sealing enable llms 10 follow expected human instruction insitgpt 59  proposed  align language models \nser intent   wide range  tisk netuning \nihuman feedback starting witha set  labeler writen prompts\n prompts submited   openal apl dataset\n labeler demonstrations ofthe desired model behavior \nsalleted gps fietned   dataset 3\n\u2018use  fumanranked move ouput  cllected  \nfinetune  model using reinforeement earning  metho\n kaown reinforcement learning trom human feedback"}, {"page_number": 7, "sentences": "im\n\n0meta\n\nfig 8 popular llm fanilcs fig 9 gps shows  larger models make inresingly\nsffcent vse  contestinfrmation  show contest\ntearing perfomance  simple tsk eguring  model 10\nremave random symbols fom word oth sith  without\n2 natural language ask description courtesy 0 6\n\nrlhe 96 shown  10  resultant insvetgpt models\nive shown improvements  trthulnes  recone \ntoxic oupit generation   minimal perfomance\nregressions  public nlp datasets\n\nfig 10  highlevel overview  rlhf courtesy  89\n\n\u201c  importa milestone  llm development isthe\n\nlaunch  chigpt chat generative petrsined trnsformet\n0 om november 3 2022 chatgpt ts chao  enables\n\u2018ers  leer  conseration  eomplete  wide range \nasks   question answering infomation seeking ext\nsummarization   caagpt  powered  opts\n later  gpt sibling model 10 instactgpt \n\u2018strained wo fllow  instruction   prompt  provide \nete response\n\nopt 33 isthe latest   powerfal llm  \ngpt family launched  match 2028 gpt  4 multe\n\u2018modal lent     ake image  ex  inputs \nfproduce text ouput stl les capable  humans\n    challnsing realworld scenarios gpt\nexhibits human evel performance  various pofessional \n\u2018kademic benchmarks including passing 2 simulated bar exam\n\u2018wih 2 score aound  top 10  test takers shown\nfig ii like carly gpt mosels gpt  fist pre \nedict next tokens  large ext corpora   finetuned\n\u2018 rlhef io align model behaviors  human desied nes\n\n2  llama family llam  collection  founds\nfon language meds released unlike opt mods\nlama model \u2019 opensource ie model weight \nreleased 10  research community   noncommercial\nteense thus  llama funuly grows rapidly  \nrodels ate widely used  many research groups  develop\n\u2018eter opensource llms  compete  closedsoutce ones \n develop ask specific llm  missional applications\n\n\u201c fist se  llama models 32  released  febru\nary 2023 ranging  7b  6b paramstrs  models\nse pretained  trillions  token collected  publicly\nsill datasets llama uses  tansformer atchitectre \ngpt   minor architectural modifctons including\n1 using  swiglu actation funcion instead  relu\n2 ssing rotary posonal embeddings instead  absolute\n\u2018poston embedding 3 using rootmeansquare yer\nhomalization instead  standatdlayernormalization \n\u2018pemsource llamai3ib model operorms  propriety\ngpt 1758 model   benchmarks making  goo isle  cla research"}, {"page_number": 8, "sentences": "fig 11 gpt pesfommance  academic  professional\ncexans compared  gpt 35 courtesy  33\n\n uly 2023 met n parinerbip  micros reese\n lama2 collection 61 hich inlade bth fudation\ntangusge meds  chat mele finetuned  dialog known\n llama\u2018 chat  llama2 chat mods  reported\n\u00a9 outperform  opersource models  many public\n\u2018enchiarks fig 12 shows  taining process  llama2\nchat  process begins  retaining llama using\nfpublily aalable oaline data ial version \ntlama2 chat   via supervised finetuning subse\n\u00abquent che mode  itatvlyfeined using rlhf jection\nsmpling  proximal policy optimization inthe rlf stage\n accumulation  human feedbuck  revising  reward\n\u2018model  crucial  prevent  foward model  \nhhnged  moch  could ur  stability  llama\n\u2018del tng fig 12 tisining  llama2 chat courtesy  6\n\nalpaca 62  finetuned rom  llamatb model wing\n52k inston following demonstations generated  \nsole  selfintruc wing gpe extdavineo03 alpaca\njs  eosteffective  taining especially  academic\n\u2018esearch   sein vation set alpaca performs\nsilly t0 gpe despite  alpaca  much salle\n\n\u201c vcuna team  developed 13b chat model vicuna\n138  finetuning llama  usershared conversations\n\ncollected fom sharegpt preliminary evaluation using gpt\n\u2018fae  evalustr shows  vicun13b achieves tore \n\u20180 quality  opena chatgpt google bad \n\u2018outperforming  models ike llama  stanford apacs\n   9  cases 13 shows  relative response\nquality  vicuna     wellknown models \ngpt another advantage  vieuna13b iit elatve limited\n\u2018computational demand  model taining teining cost \n\u2018cna 138  merely 300\n\nfig 13 relative response quality  vieuna anda  \nwellknown models  gpt couresy  vieuna team like alpaca  view  guanaco modes 63  also\nfnetuned llama modes using instruction following dts fining  done  efciely using qlara \n rnetuning  6sb parameter model   done \nsingle 48gb gpu qlora buckpopapates gradients tough\n4 oven hit quantized pre language mode low\nrank adapters lora  best guanaco mode ouperforms\n previously released models   vicuna benchmark\nfeaching 993   performance level  chatgpt \n\u2018 reiring 24 hours  fine uming  single gpu koala 64  yet another instruction following language\n\u2018model built  llama tut wih specifi focus  interaction\n\u2018ata tht ichude user inputs  responses generated  big\n\u2018capable closedsource chat models   chatgpt koala38 model performs competitively  stateoftheant\nchat moves according 40 human evaluation based  fle\n\u2018word user romps\n\nmisua7b 65   7bparmeter language moet engi\nneered  superior performance  efficieny mistal78\n\u2018outperforms  best opensource 138 model llama2138\ncrow  ealted enchmarks   best pensoure\n4b model llama34b  reasoning mathematics code\nsneraton  model leverages eroupedquery attention \nfaner inference coupled  sling window tention 10\ncfctvey handle sequences  arbitrary length  rece\ninference sox\n\n\u201c llama family  rowing rpidly   instrction\nfollowing models   built  llama llama\n2 including code llama 66 goria 67 girae 68 vigosne 69 tulu 658 70 long llama 71  stable\nbeluga 72   mame 43  palmt family  palm pathways language\n\u2018modet family  developed  google fist palm\n\u2018model 31  announced  ape 2022 apd remained private\n\u2018unl march 2023 tes  40b parameter tansformer based\nilm model  pretrained   highquality ext corps\nonsising  78d halon tens  comprise wide range\nfof mtu language tasks  use cases palm  petsned"}, {"page_number": 9, "sentences": "fon 6144 tpu v4 chips using  pathways stem \nfables highly ecient taining across mulple tpu pods\npalm denonseates continued benefits  scaling  achiev\ning stateoftheartfewshot tearing resus  hundteds \njanguage understanding  generation benchmarks pal \n5408 outperforms   sateotthe hinetuned models\nfon  suite  mulstep reasoning asks ut also om par \nihumans   recely released bigbench benchmark\n\n\u2018 upalm models  sb 62b  s4ob scales \nontnll tained  palm  ul2r 4 method  cotinine\ntsning llms ona  steps  ul2 miste denoser\nobjective 73 approximately 2x computational savings\n\u2018tei reported\n\nlupalm  ater instruction intuned  fan palm 74\ncompared   snsrcton finetuning work mentioned\nhove fan palm lineuning  performed using 4 much\ntanger number  task larger model sies  chai\ntought data asa revul flarpalm substanillyouperfonns\nfpeviows instuctionfollowing models far insance flan\npalmes40b  ie ieatuctionfnetuned oa 18k tsk\n\u2018outperforms palms40b   large maypin 94 oa \nrae fintuing dats compass 473 datasets 16 tsk\n\u2018uogevies  1836 total isk  luseated  fig 1\n\nfig 14 flanpal mfncunig consist  473 datasets  \nask categories courtesy  74\n\npalm2 75  8  computeeficient llm  bet\nter multilingual  reasoning eapalites compared  \n\u2018edocesor palm palm2  tained using  mixture \nfbjetves though extensive evaluations  english mull\ntingual  reasoning tasks palm2 sgnficanly improves\n model perfomance  dowasueam tasks acoss diferent\n\u2018model sizes  simultaneously exibiing faster  \n\u2018fen inference  pal\n\nmedpalm 76 isa domainspecific palm   de\nsigned  provide highquality answers  medical questions\n\u2018med palm  finctoned  palm using instuction prompt\nfuing \u00ab paramcteefficient method  aligning llm \nnew domains using afew exemplars medpalm chains \nocouraging rests  many beathcare asks although iis\nsinfenorto human clincans medpaum 2 improves med palm via meddomain fneturing  ensemble prompting\n\n77 megpalm 2 scored  10 865   medqa\ndate ea benchmark combining si existing open ques\n\n answering datasets spanning professional meal exams\n\u2018esearch  consumer queries improving upon medpalm\n  19  seting new state athe\n\n6 representative \n\n ation tothe models discussed   previous sub\nsections tere ar alee popular llms  jo pt belong\n thowe thes model famies yet  hive achieved est\nperfomance   pushed  llms field forwards \nbriefly desere  lem   sbsecton flan  98 wel otal explted  simple method fr\nimproving  zroshotiaring alts  language model\n\u2018 showed tht instruction toning language mode \ncollection  dance described via instructions substantially\nimproves zeroshot performance  unseen asks take\n4t37b parameter pretrained language model  insrton\ntune ton  \u00ae nlp dstasets verbalize va natural language\ninraction templates  ell ths instoctiontned mode\nflan fig 15 provides comparison  instruction tuning\n\u2018wih prerainbnetone snd prompting\n\n15 comparison  insruction caning  pre\n\nfinetane  prompting courtesy  78 gopher  79 rue etal presented  amlysis \n\u2018transfomerbased language model performance across  wide\nrange  model sean  models uth ens  millions \npattmeter  1 280 billion parameter model called gop \u2018 models  evaluted  182 diverse asks ahicting\nsue oftbean perfomance ares  majority nue\nfof layers  heyvalve size hyper parameters \n\u2018ferent ode sies ste shown  fig 16 fig 16 mods architecture details  gopher  diferent\n\u2018number  parameters courtesy  78 40  80 san et al developed   system  easly\n\u2018mapping  natural tanguage tsk jnto v human eadable\nromped form converted  tage set  supervised\natsets cach  maliple prompts  diverse wording"}, {"page_number": 10, "sentences": "\u2018 romped datasets allow  benchmarking  ability\ncof model  perform completely hek tasks 2\n\u2018 eacoderdecoder model  developed  consume tetial\ninputs  produces target responses model  aie \n4 multitask mixture  nlp datsesparitoned ino diferent\ntsk\n\nernie 30  81 sun et al proposed united frame\n\u2018work named ernie 30  pretaninglaresale knowledge\nhanced model tasesautoregresive network nd sul\nfnceing network  tha  taned model en  east\ntere  bath natural language understanding st generation\nseks using reo sot lamin  shot learning  te\n\n\u2018  trained ernie 30  10 hilbon paramters\ncon 4 tb corpus consisting  plain texts anda large sale\nknowledge graph fig 17 lattes  model arhitccure \neimie 30\n\nfig 17 high tevel mode architecture  ernie 80 couresy\n 8 retro  82 bocpoaud el eahanced autoregressive\nlanguage models  conditioning  document chunks 1\n\u2018wove fom age corps based  toa sini  pre\nceding tokens using a2illon token database  retreal\nenhanced transformer retro obais comparable perfor\n\u2018mance 9 gpt3  jurassc 83   pile despite using\n25 fewer parameters shorn  fig 18 reto combines\n4 frozen ber retriever differentiable encoder  chunked\n\u2018rossatention mechanism  predict tokens based   order\nff magnitude  data    typically consumed\nuring taining\n\nglam  84 du etal proposed family  llms\nramed glam generalist language model  use\nsyasely activated mintreapens architecture  scale \n\u2018model capacity  also incuing substantially lens aiing\n compared  dense variants largest glam  12\ntellion parameters whichis approximately 7 larger  gpt\nikconsumes  13 ofthe energy sed  tun gpt \n\u2018equies bal ofthe computation fps  inference  till\nseeving better cverll tro one  feshot perfomance\n\u2018oss 29 nlp tusks fig 10 shows  bighlevel architecture\nspglam\n\nlamda tn 85 thopilan otal presented lamda family  transformer hased tral langage modes special\njaed  dialog     137b parameters apd ae\n\u2018rearained  1s6t words  public dal data  web ext\n\nfig 18 reto architecture left simplified version  \nsequence  ngth n 12  split   3 chunks  sie\n 4   chunk  r\u00e9tcve k 2 neighbours \n\u2018stokens revieval pathy  shown ontop right\ndetails ofthe interactions inthe cca operator cats \n\u2018maintained  neighbours ofthe fist chunk  fet  lst\ntoken   gt chunk  tokens   second chunk\ncourtesy  82\n\n19 glam model architecture bach moe layer \u00ab\n\u2018otto block  inerleaved wath  transformer layer \nupper block courtesy  4\n\n\u201c showed  ietuning  nnotted data  enabling\n model  const extemal knowledge sourees  lea\n\u2018sanitcan provements towards  two ey challenges \n\u2018afer  fatal rounding\n\noft  85 zhang etal presented open pretrined\n\u2018transfers opt site  decoder  retained ane\nformar ranging irom 125m  17sb parameters  \nshare  researchers opt models\u201d parameters \n\u2018shown  20\n\nly\n\neveeeead\u00e9\n\nfig 20 ditfxent opt models\u2019 architecture details courtesy\n 86"}, {"page_number": 11, "sentences": "chincha  2 hottmano etal ivestiatd  optimal\n\u2018model size  number  tokens  tning transformer\ntonguage model  2 given compute budget waning\ncover 400 language models ranging  70 milion  \n billion parameters    300 bilon tokens  found\n  ompateopimal training  model sie  \n\u2018number  waning tokens   scaled equal  every\n\u2018doubling  model size  numberof tuning tens \nko  doubled testo tht hypothesis  taining \ndredicted computeoptimal modcl chinchilla  uses \nsane compute budget  gopher tat  tob parameters \n39 moe  dts\n\ngalactica  87 taylor tal iuoduced galactica 2\nlage language model hat  str combine  reason bout\nscienic knowledge tained  alge siete corps\n\u2018 papers reference material knowledge bases  many \nsores galactica performed well  reasoning outperforming\nchinchilla  mathematical mmlu  413 10 357 palm s408  math  score  201 vers 885\n\nccodegen  88 nijkamp et  tained  released\nfamily  lage language models  wo 1618 parameter\ncalled codegen cn aural language  programming\ntanguage data  open sourced  waiing ibary jax\nformer showed  utity   tained model \nemonstating  itis compete   previous state\nheart  zeroshot python code genestion  humana\n\u2018 futher investigated  mulistep paradigm  prograr\nsgmhess single program  facorzed  mult\nple prompts specifying subproblems also constuced\nfn open benchmark mulitur programming benchmark\nmitpb consisting  15 diverse problem sls  \nfactorze  lit prompts\n\nalexatme  99 solan et al demonstrated  male\nlingual largescale sequencesequenee seq2seq mode\nfpretained   mixture  denoising  causal language\n\u2018modeling clm task afe  ecient sbot leaoers\n decoder models oa arious sk  tained 2\n20 bilion parameter mullingalsey2seq model called alexa\n\u2018teacher model alegttm 20b  showed   achieves\nstate heart sota performance  shot summarization\ntusks outperforming  larger s408 palm decoder\niodel alexatm consis  46 encoder layers 32 decoder\nayers 32 alention heads  ait  4090\n\nsparrow  90 glase etal presented sparow \nnformationseeking dialogue aget tained tobe moce hep\n\u2018concet hanness compared  prompted language model\n\u2018belies  used einforcement luring  hun feed\nback  train  models  two new addins  help\n\u2018human rates judge agent behaviour highlevel pipeline\n sprow model  shown  fig 21\n\nminerva  9t lewkowyer et introduced miners\n4 large language model pened general natural language\n\u2018btsand  ined  echnical conten tackle previous\nlem srugele  quantitative reasoning   sling\n\u2018mathematics scence  engineering problems\n\nmod tn 192 tay ot al presented  generalized \nmifed perspective fr selsupervision  nlp  show \nsitfrent pretaining objectives   east  one another\n  interpolating  diferent objectives  \n\nfi 21 sparow pipeline relies  human partition 10\ncontinually expand training st courtesy  90\n\neffective  proposed minturedenvisers mod pre\n\nning objzctive  combines diverse pre training paradigms\ntogether  framework  known unifying language\nearning u2 overview  ul2 pret\n\nkook et\nf fig 22  overview  ul2 pretsiningpuragm courtesy\n 921\n\nbloom  93 sea eta presented bloom 3 1755\nframeter openaccess language\u2019 model designed  built\nhanks collation  fundede  researchers bloom\nfit decederonly tansormer language mode aed  \nroots corpus\u00bb datset comprising hundreds  soutes \n46 natural \u2019 13 programming languages 89  wt ar\n\u2018overview  bloom atchitecture  shown  fig 23\n\n\n\nhee\n\nfig 2  overview  bloom architect courtesy \n198\n\nglme  94\n\nzeng ot  intoduced glmi3ob 9"}, {"page_number": 12, "sentences": "\u2018lingual english  chinese preined language made\n\u2018 130 billion parameter atempt  opensource\n2 100bscale mee  east  good  gpt dain \nuve  models   scale   sucessfully \nteined\n\npythia  9s bierman et introduced pythia  suite\n 16 llms laine om pubic data seen inthe exact \n\u2018order ad ranging  size  70m  t2b parameters \nfrovde pbc access  154 chockpoits  tach one  \n16 model aloagside tools  download  reconstuc thee\nexact waning datloadrs fo  sad \u2018orca  96 mubherje eal develop orc 13blion\nparameter model tht leans  imitate  resoning process\nff large foundtion models orci learns  ich signals\nftom gpt inclding explanation races stepstep ugh\nroceses  eter complex insinctions goed  teacher\nesstance  chatgpt starcoder  97 li et al induced starcoder \nsurcoderbase ae 1556 parameter models  8k\ncontext length ining capabilites  fast lrgebatch ine\nterence enabled  multgucry attention stacoierbase \ntrsined one tron tokens sourced   stick \ntags collection  permisively licensed github repositories\n\u2018 inspection tools   opout process finetned\n\u2018starcoderbase  35b python tokens resulting inthe eration\n slaconer  performed   comprehensive eal\n\u2018tion  cade limb  date  showed  strcoderbase\n\u2018utperforms every open cade llm tht suppts mule pro\nramming languages  matches  outperforms  open\n\u2018sodecushmany001 model\n\neve general mais lear  contest fe sot \nfollow insrstions zerosho specify  trained\nkosmos1  scrteh  webscale mltmoda corpo\ninciaing atari intereaved text  image imageaption\n\u2018pi  ext dts experiment rests show  kosmos\ntachieves impressive performance   language understand\ning generation  even ocreiree nlp tirectly fed \nacumen images gi pereption language tsk ieloing\n\u2018string   vision tasks  3b image recognition sith\n\u2018esciptons specifying casication mia text insaco\n\n\u2018gemini  99 gemini team intovced anew family \n\u2018nukimodal moves  exhibit promising capabilities across\nimage aio vdeo  text understanding cerin fay\ninciaes tree versions\u2019 ul  biglycomplex tasks pro\nforenhanced perormance  deployabiity  sale  nano\n devce applications gemini arciteetre  built ontop\n\u2018 transformer decoders adi ined  support 2k coment\nteng ia using efficent sttenon mechan ofthe oer popula llm frameworks  ecnigues\nsed tor efficient developments  lms includes inee\n\u2018monologue 100 megutontuing nlg ioi longformer\n102 offiml 103 metalnt 104 dromedary 108\npalmyra 106 camel 107 yam 108 mpt 109 orca\n2110 goria 67 pal iti claw 1121 codegen \n13 zepiye 114 grok 115 quen 16 mamba 0\nmistral887b 117 docllm 118 deepseckcoder 119\n\nfusellmtb 120 tayllama11b 121 llamapeo sb\n2\n\nfig 24 provides  overview     mos repre\nsentative llm frameworks   relevant works tht fave\n\u2018omtbated tothe sbewest  llms  elped  ph \ntims  llm\n\n  llms ane built\n\n  section  ist review  popular achtetares\nsed  llm ante discs dat  ming techies\nranging  ta preparation tokeization  pretaing instruction tuning alignment\n\n  move aehitecare  chosen  major eps\ninvolved  wating  lem inclodes data preparation coe\niecion cleaning deduping ete tokeizatonr model pe\n\u2018ining ia  solfsupervised learning fashion insirvtion\ntinge  alignment wil explain spat subsection sp fe also stated\nip 25 dominant llm architectures\n\n\u201c  widely ued llm architectures  encodeony\necoder apd encodedacoder    ate based ot\n\u2018teansfomer asthe building blk therefore  also view\n transformer architecture hee\n\n1 transformer  sgroundweakng work 4 vasa\nta propoued  transformer fanewor   oig\ninally designed  effective parallel comping using gpus \u2018 heart  transformer   elfjatention mechanism\n\u2018  capture tongterm contextual information much\n\u2018 effectively using gpus  th recurtence ad convo\niuion mechanisms fig 26 provides  highevel overview \nteansformer work  ths section  provide  overview ofthe\n\u2018main elements  variants se 44 123  move deta\n\n\u2018 transformer language model architecture originally\n\u2018ropoted  machine translation consists ofan encoder ata\nbdecoder  encoder  composed   ack  n6\nidentical transformer layers layer  two sbyer \u201c fist one isa multihead sellatention lay   ther\ncone  simple posionwise filly connected feodfrward\nfetwork decoder  composed   stack  6 denial\nlayers  aditon  de wo sublayer   encoder layer\n decoder  hd subhayer  performs multihead\nlletion   output   encoder stack te tention\ntupeton   described 3s mapping aque   set  key\nvalue pairs   output whore  gry kes vals ad\n\u2018utp areal vectors te ouput computed asa weighed\nsum   values   weigh assjned   value\n computed   compatibility function ofthe query wit \nconesponding key stead  performing  single ateaton\nfuneton  dye mensional keys values  queries\nitis found   beneficial t0 linearly projet  iets\nys  sales wth diferent lee linet projestions 1\ndtd add dimensions respectively psion ending \nincorprated  fase information   relative wr absolute\nposton   fen   seqience"}, {"page_number": 13, "sentences": "ee\n\nsemses cn \n\n iit eo \n\nvet tm noe tarr tt\n\n ors errs marr cor  \n\u2018ciasrraore 2018 2020 203 2022 2023 2023\n\nee\n\n\n\nein woe\n\ncame  \n\nwer ena\n\nfig 4 timeline   ofthe  representative llm frameworks  ar addition  large language models  \n\u2018parameters threshold  ical   representative work  pus  ints  langusge modes  paved  ay\ntor thelr success eg vanilla transformer bert gpt1 a5 well   smal language models shows enites  seve\n   models  also  approaches shows oly approaches 2 encoder fo  fail teach stage  ten\ntion layers  access   words   inital sentence\n\u201c presraining   mdse ually consist  e\n omupting given sentence  instance masking\nrandom words    tasking  model  finding \ncontacting  initial sentence encaser meds  eet\n taske requiring andentanding   fll sequence\nsich  sentence casifation nan entity recognition \nextractive question answering one prominent eneader oy\n\u2018model  bert bidirectional encoder representations fom\n\u2018transformers proposed  24\n\n53 deeoder   meds   tage  \n\u2018wot  tention layers   access  words positioned\n  inthe sentence  models ae also sometimes\ncalled atoregresive modes preaning ofthese models\n usually formulated  predicting  next word  token\n  sequence decderonly models  best sited \ntasks involving text generation gpt models  prominent\nvamp ofthis model catezoy\n\n4 bncoderdecoder  models use  encoder \nevoder   sometimes called sequencetosequence mod\ntls ateach stag theatenton layers ofthe encode ean acess\nsil  words   intial sentence whereas  tention\nlayers ofthe decoder  acceses  words positioned \n4 given word   input  models  usually pre\nrained sing  objectives  encoder  decoder models \n\u2018sully involve someting abit  complex instance\n models ae prerined  replacing random spans  text\n  conan several words  single mask special\n\u2018word   objective    predict  txt tha \n\n\u2018mask word replaces encoderdecoder models  best sited\ntor tasks  generating new sentences conditioned  2\nsken int sich se summarization rasstion  generative\nquestion sneering\n\nb data cleaning\n\ndaa quality  crucial 0  performance  language\n\u2018models weined data clening techniques suchas\nhering dedupliction ae shown    ig impact \n model performance\n\nasan example  faleond0b 124 pend et showed\ntat propery ere  dedupicated web data sone cn ead\n powerul models even significantly outperforming models\n  sateotatsned   pe despite extensive\niherig hey wee ble oi fe tion hey om\nccommencral  abo teleased  extact  00\n\nstou fom  rerinedaved data  178 pan\nters language models tained oni 27 shows de refaement\n\u2018ocese  commoncrawl data   work\n\n1 dora filtering daa fering aims  enhance  gual\nity  training data   efecuvenes   tained lems\n\u2018common data iltering techniques include\n\nremoving noise refers  climinating relevant  noisy\ndata tat igh vac  mds ity oases wel\n\u2018  example one san think  removing false information\n\u2018rem  traiing data  lowe  chance  move generating\nfalse responses two mainsteam approaches  quit hite\ning includes classierbased  reurstibased frameworks"}, {"page_number": 14, "sentences": "eetiningadinercon ng \u2014\nmee pe\name ss\ndiet sing\n \n contin angen\n\u201chapatoa \u00abcompson\n\nfig 25  figure shows diferent components  llms"}, {"page_number": 15, "sentences": "fig 27 subsequent stages  macrdata refinement remove\nrneily 90   documents originally  common\n\ncourtesy  124\n\nhandling outliers wenilying  handling otiers \nsnotalies   data  prevent   dspropertontely\njiuencing  mee\n\naddressing imbalances balancing  distibtion \nclasses  categories inthe dataset  avoid biases  ensure\nfar representation specially useful  responsible\n\u2018model tuning  evaluation \u2018text preprocessing cleaning  standardizing text data\n removing stop words punctuation   elements \n\u2018sy  conte sigiicnly   mode earning\n\ndealing  ambigultes resolving  exchoding \nbiguous  conuadctry dats  might confuse  model\nusing waning help  model  provide \n\u2018inte  reliable answer\n\n2 dedication dedoplcation reer wo  process \nremoving dla instances  repeated excamences  \n\u2019 daa  dataset duplicate data points  introduce\n\n\u2018biases inthe model taining process  reduce  diversity model may lum fom  sume examples multiple tines\npotently lading  oveviting  thse patil stances\n\u2018 works 125  chown  deduplication improves\n\u2018model ability  generalize 10 new unseen dat \u201c deduplication process  patiulary important \ndealing sith ange datasets dapiats  unintentionally\njnflate  importance  erin patems  characteris\n\u2018thin  espectally relevant  nlp tasks  diver \nsepresetative waning data cuca fr bling sobs lane\nsage models\n\n\u201c specific deduplicaton method  vary based \n nature   data tnd  requirements   particular\ntanguage model  waned irma involve comparing entire\ndats pots  specific fentures  meni  limite \ndlictes dosent evel existing works mn rely \n overlay rato  ighevel features eg mrams overlap\n\u2018 documents  dtet duplicate samples tokeizations\n\n\u2018tkenization wefetes   process  comering  se\nquence  text  smaller parts known  tokens simplest tokenzation tol simply chops text  tokens\n\u2018nse  whitespace  tokenizaton toes rely  3 word\naictonary however  vocabulary oov  8 problem\n thie cave   tokenizer  knows words  ite\naitionay increase  coverage  ditonais popular\ntokenizer used  llms  sed  subwords hich n\n\u2018 combined  form large numberof wor ineluding \n\u2018words unseen  waning data  words  irene languages\n  follows  describe tree popula okenizes\n\n1 byepairencoding byepairencoding  oxgially 2\n\u2018ype  data compression algorithm tat uses feguent patterns\nbyte level fo compress  dst  definition thi algorithm\n\u2018mainly esto keep  frequent woes  thet orginal frm fn bresk  ence tht  common  simple\nparadigm keeps  vocabulary   large aso good\nenough  represent common words    time alo\n\u2018morphological farms ofthe fequent words canbe represented\n\u2018 well sufi  prefix  also commonly preseted inthe\nttsning dit   slgonitim\n\n2 wordpieceencoding  sgoitim  mn used \n wellknown models   bert  electra \u2018beginning  taining  algorit aks  te alphabet fom\n taining daa  make sue tht nothing wl eles unk\n\u2018 unknown rom  taining dataset ease happens \n model  given  input  cannot  tokerized  \ntokenizer mostly happens  cases  woe characters \nrot takenizable   similar  byepairencnding  tes \n\u2018maximize  lkeibood  pating  tbe tokens  voeabulary\n\u2018sed  thei regency\n\n5 sentencepiecebncoding altvough  tokenizer de\nscribed   stong   many advantages compared\nfo whitespace tokeization  sll take assumption \n\u2018words  lays separated  whitespace  ranted ssumpion sat aways  inact  se languages words\n  corrupted  many nosy elements   unwanted\nspaces  even invented words senteneeieceencoding tries\n adres  ise"}, {"page_number": 16, "sentences": "positional encoding\n\n1 absolute postional embeddings ape 44 hasbeen\nsei  eriginal transformer model 1 preserve  ine\n\u2018ation  sequence onder therefore  postal information\nff words  aed   input embeddings atthe botiom \n\u2018oth  ender  decoder stack various options\n positional encodings ether iarned  faed inthe vanilla\n\u2018transformer sine  cosine functions  employed fortis\npurpose main drawback  using ape  transformers\nfete restriction toa certain numberof tokens additonal\nape fast acount forthe eave distances betwen tokens\n\n2 relative posiinal embeddings rpe 126 involves\nextending selfattention  ake ita account  prise hak\nsetween input element rpe  added   model  160\nlevels fist   addonal component   kays \nsusequely   subcomponent   ales matic \napproach looks   input   fllycomnected graph \niabee  deta edges nthe case  liner sequences edges\n\u2018 cape information   slave postion diferences\n input clement clipping distance represented  k\n22k  m1 species  maximum limit  relative io\n\u2018aions allows  mode  make reasonable predictions\n sequence lengths  sre  prt   traning dt\n\n3 rotary posion embeddings rosary positions em\n\u2018oddiag rope 127 cles problems  existing ap\n\u2018roaches learned absolute sional encodings  lack gen\nalizaityand meaningfulnss particulary  sentences\nse shor moreover cent methods like  postions\nembedding face challenges  constucting 4 ful tation\n\u2018mati betwcen posiions rope uss  roulion mati \ncede  absolte positon  words  simuitancousty ine\n\u2018slugs explicit elative positon deal  slfatention rope\ntings veil features ke flesibity wit setence lengths 8\nevrease  word dependency  reative distances increase\nland  abiiy  impeove linear seattenton  relative\nposton encoding gptneox 208 palm codegen \nlama sre among models  take adantage  rope \n architect 14 relative pestonal blas  coves bebid  spe\n positional embedding  10 fciliste extapolaton dung\ninference  sequences longer tan thse encountered ia alae\ning28 pres et  proposed arena wih linear biases\nalb instead  simply adding positional embeddings 0\n\u2018word embeddings intoduced abs othe tention sores\nfof querykey ps imposing  penalty proportional  \n\u2018distance inthe bloom mode alb leverage\n\ne model presaning\n\npreraining isthe  first tp  inge language model\ntwining pipeline   helps llms  acquire fundamental\ntanguage understanding capabilites  ean  useful ina\n\u2018wide range  language elated sks reining tlm ie trained   massive amount  astlly unlabeled\ntexts usally   selspersised manner diferent\napproaches used  pretaning like nest sentence prediction\n241 wo  common ones ince next token preition\nautoregressive language modeling masked tanguage\nrodeling autoregressive language modeling framework given\nsequence af tolens rr ty  model wes  pret\n\u2018ext token  smeties next sequence  tokens \nfn aufoegrescive fasion one popular toss function sn \nse isthe loglikelihood  predicted tokens  shown  eg\n\n\nlrunleteeryasiontives \u2014\n\ngiven  auworegressive nature   framework \necodenonly modes  atrally beter sited  learn \n accomplish  task\n\n masked language modeling  words  masked\nina sequence andthe model  aid  predict  masked\n\u2018wotds based onthe surounding coatet sometimes people\nefer wo  approach  denoising sutoencoding 8\n\u2018denote  maseacomped samples inthe sequence 4\n  training objective ofthis approach   writen 3s\n\nzula  sons\n\n  recently mixture  experts moe 30\nlu3h  become  popular  lem space moes\nenable models   peuaind  much tess compute\n\u2018hich means one deamatcaly scale   model \nlauset size wit   compute budget asa dense mod\nmoe consists  fo main elements sparse moe layers\n\u2018  ured insta  dense feedforward network fen\niayers  hve  certain number  \u201cexpens\u201d eg 8 im\n\u2018  expert  neural network  practice experts\nte fen  hey  also   complex networks pate\nnetwork  router  determines  tokens resem\n\u2018 exper  worth noting one  send  token\n   one expert   route  token   expert\nfe one   big decisions  serking  mos \nrouters composed  learned parameters   pretained \n  time  ret ofthe network fig 29 provides sn\nistration nf  saitch transformer encode bock  \nted  moe\n\n fneuning  instruction taning\n\narly language models   bert waned using self\nsupervision  explained  section hie  notable 19\n\u2018etform specific tks  onder   foundation made oe\nful ned  e finetuned  spect task  labeled\n\u2018ata alled supervised finetuning  sft  abort por\nexample nthe orginal bert pope 24  mode  fine\ntuned  diferent tsks   rece lms  longer\nrequire inetuning   used   still beef rom tsk\n datspectc finetuning example open reports \n much smaller gp 8 tavo model ean oopertorm gpt\n\u2018 fine tne  ask specie dts \u00a9\n\nfinetning   need   performed w  single\ntask though  thee ate diferent approches 1 mlitsk\nfnetuning ee eg mahabi eta 132 fine uning  one\n\u2018  tks  known  improve resus ao reduce \nomplenty  prompe engincering serve  \n\nta es titers"}, {"page_number": 17, "sentences": "bak ples\nse x7   yy\n\nrelive poston ebeling\n\n\u00a9 rotary poona embeding 127\n\n4 relaivepositons bis 128\n\nfig 28 various positional encodings ae employed  llms\n\nfig 2 iosraion  switch transformer encoder block\n\u2018 replied  dense feed fora network fpn laye\n\u2018resent inthe transformer  9 spare switch fen layer\nlight ble  courtesy  31\n\nltemative  revival augmented generation fushermore\n ae ther easons   might  advisable  hnetune\n example ne might want  netune  expose  mel\n new  feopicary data      exposed 1\n pretsining important reason  finetune llms ist align \nresponses othe expectations humans    proving\ninstructions though promis isthe called instruction\ntuning 133  dive   dele  fow  design\nsn engineer prompts  section ivb byt   coment\n instution tuning   important  uodersapd tht \ninsructon  2 prompt  specifies  task   llm\n accomplish instruction tuning datasets   natural\n\ninsrvetions 134 nclade  ony  ask dein bat ter\nononet sa psec  hig\n\n\u2018 specie approach  instetion dasers used 19\n\u2018nsuation tune  llm vies  genecally speaking ine\nstrution tuned models outperform weir orginal foundation\niodele  ate based   example inructgpt 59\nfutperforns gpt   benchmarks ue\n alpaca 62  compared  llama seletnstrut 135 proposed  wang etal  abo \npopular approach along tis lin    induced \nframework  improving  instivctonfollowing eapabiics\n pretained tanguage models  bootstrapping thei \ngenerations tei pipeline generates instuctios inp ab\n\u2018utp samples fom  langage model  fies invalid \nsar ones  using  ofne tne  original med g alignment\n\n alignment isthe process  steering al systems tomards\njnuman gous preferences apd principles llms preuned\n word prediction often extbit unintended behviens example  might generate contents  ae exe ara\n\u2018misleading  bse\n\ninsaco tuning discussed  gets lms 2 step\nose 1 beng algne however n many cass itis important\n inclue  steps  improve  alignment ofthe mse\nsind oid unintended behav  review  mos popolar\n\nreco ap toch elna 0 rat"}, {"page_number": 18, "sentences": "approaches 1 aligment   subsection\n\nrlu reinforcement earning  human feedback \nrlaif reinforcement leaming trom  feedback  8\nopulir approche lh aves  rewar mods learn\nsennen  homin fecdhack  reward model afer\n\u2018ing tne iv able  rte differen onpats  score ther\nscoring w thir alignment preferences given  humans fesvard model gives teedhack   original llm  \nfeehack used  tune  lm futher 137 reinforcement\nlearning fom al fedhack onthe  hand dry connects\n4 pretrined  welaligne model   llm ad bel\nollearn fom larger  mote aligned models 138\n\n anotier recent work known  dpo 139 railow\ntal discussed  rlhf isa complex  often unstable\n\u2018proceed ried  adaress    new approach teverged  mapping  reward functions  optimal\npolices  show tit thi consiained reward maxinztion\nproblem canbe optimized exactly  single stage  policy\ntaining essentially solving  clasifcaton problem om \ninuman preference data  resulting algorit  \ncalled direct preference optimization dpo stable pee\nfoxman  computationally phtweightcimiating  need\nforiting area med sapling rom  lm  ie\n\u2018tuning  pestonning seniieathyperparametr toning \u2018bsersed tha netaing sith dpo exceeds rlf aii\n\u2018oni sentiment  generations nd imposes response ality\n\u2018 summarization fig\u2019 30 shows  highlevel comparison\n\u2018tween dpo  rlhe\n\nfig 30 dpo optimizes  human references  avoiding\nreinforcement iaming existing methods  fintning lan\nftuage models  himn feedback fst ht reward mel\nova dataet  prompts sod human preferences  pus \n\u2018esponser   use rl  finda policy  makimizes\n teamed rewatd contrast dpo dicey optimizes \n pliy best satstying  preezenes  simple classi\nfetion objets without  explicit reward function oe rl\ncourtesy 0f 139 even move ecenly ethyarah et al proposed anew lig\n\u2018ment spproich called  kabnemantverky opdaizton\ngto 136 unlike existing stateart approaches kto\ndes  requite pated preference data 2 ju ad \nfonly needs 3 apd knowledge  wheter  fs dsiable \nlndesiable ktosaligned models  shown 16  good \neter  dpoaligbed modes  scales trom ib 10 308\nespite pt using pared preferences kto  aso far easier 1\n\u2018se inthe real world  preference optimization methods \n kind  data  neds  far  abundant example\n\u2018tery real company ae lt  eustomer interaction dts nd\n\u2018whether  interaction wae sccesfl eg purchase mae\n unsuceesfl eno purchase made however  \ntile   counterical data   would  made\nsin unstecessolcastomerineraction nit sees one\n\nsy fig 31 shows  highevel comparison  kto \nnr alignment approaches discussed fig 3 llm alignment involves supervised tnetuning fo\nlowed hy optimizing 8 human centered ns halos cher  pated preterences  existing approaches ned \nfandtootain  conase rto uses ar move abundant\nkind  data making  much easier  use inthe ral word\ncourtesy  136 1h decoding strategies\n\ndecoding eer 10  proces  tat generation using pre\ntesned llm given  input prompt  tokenizer translates\nich token inthe input txt   coesponding token td \u2018  language mods uses  token ids  input std\nfredics  next  likely token ora sequence  tokens\nfinally model geneates logs ae converted\nrobabiles using 4 softmax function different decoding\nstrategies   proposed sone ofthe mest popular ones\nfare greedy sarc beam search aswell se ferent sample\n\u2018echnigues sich a6 topk topp nucleus sampling 1 greedy search gredy serch thes   probable\ntoken   step   next token nthe sequence carding\nllothe potential options imagine hiss simple\nspproach \u2019  toose 8 lot  tempol consistency sd\n\u2018oherency   considers   probable token st cach\nstep without considering  overall effet   sequence\n\u2018tis property makes  fast  also means  ican miss\nfuton eter sequences  might  appeared  sighly\ntes probable next tokens\n\n2 beam search unlike greedy search   considers\n\u2018 next  probable token beam search aks ito account\n n  likely tokens  n denotes  number \nteams procedure  repeated   predefined maxi\n\u2018rum sequcnce tenth  reached   endsoquence token\nappeats al  poimt  sequence  tokens aka bean\u201d\nsith  highest overall score ts chosen   outpt \nfxample  beam size  2 maximem length  5\n peam search needs 0 keep tack  2\u00b0  32 possible\n\u2018sequences 8  computationally intensive  greed\nseach\n\n53 topk sampling top sampling   technique \nses  probly distribution generated hy  tanguage\niodel  selet  token randomly   k  likely\n\nsuppose  hive 6 tokens  b ey f  ed\n pia 30  pb 20 p pd pib par"}, {"page_number": 19, "sentences": "123 top sampling tokens c de f ae disepuded\n  model outputs  60s   tne   ah \n time tis approach ensues bat  pioiize  \nrobb tokens  ineducing  elemeat  randomness\n  selection proces\n\n\u2018 randomness  usualy introduced via  concept \ntemperate  femperatre tia psrameter thot rnges \not   affects  probabilities generated   soften\nfunction making   likely tokens mere invent 1\npracice  smly consists dividing  inp lois \nchperature vale\n\nsoftest \u00b0\n\n low temperature seting signitcanly alters  proba\nbility distribution   commonly used  text generation\n0 contol  level  \u201ceeativity\u201d   generated op\n  large temperature prioritizes  tokens  higher\ndrobabliestopek  cestive way  sampling   \n\u2018sed along  beam search  sequence chosen  top\nesampling may    sequence  highest probability\n beam seach important  remember  highest\noes   alays tea ty  realise  meaning\nseqnences\n\n4 topp sampling topp sampling aso known 36 \ncleus sampling ake lighten approach rom tpk\nsampling instead  selecting  tp k moat probable tokens\n\u2018cleus sampling chooses \u00abcu value p sch thatthe sum \n probably   selected okens exosedsp forms\n2 aucleus\u201d  tokens fom   randomly choose  next\noken words  topp sampling  language move\nexamines   probable tens  descending oder \nssspe adding    lit  th stm  probabilities\nsurpasses  desholdp imagine tis could \neter specially  scenarios   tpk tokens   \n2 lage probity mass unlike tpk sampling  number\n\u2018 tokens included inthe nucleus sampling   fed th\n\u2018araiity often rests    diverse  crstve \n\u2018making nucleus sampling popolar  text generation related\ntks\n\n1 cost epecive daninnfvenceadapaton\u2019compression\n\nn  pr  review    populist approaches\nsed   centfienly compte frends\nsn usage  llms\n\nveloped  opumized tuning  llms  \n ofthe prominent ons\n\nzero  140 rajbhandai etal developed  novel\nsolution zero redundancy optimizer zero 10 optimize\n\u2018memory vasly improving tring speed  llms \nincreasing  model size  canbe efcently rained zero\ncliminaes memory redundancies  dia  modlprale\neining  retaining low communication volume  igh\ncomputational granular allowing one  scale  model\n\u2018sze proportional othe mamber  devices  stained high\n\u2018fen nodes\n\nwky  141 peng et al proposed \u00ab novel model\nsavhiccue recepance weighed key value rwi dat\ncombines  ecient pliable uaining  transformers\n\u2018withthe ecient inference  rnns  proach leverages\nlinear tention mechanism abd allows   formate \niodel ss ether \u00abtransformer   rnn  prallizes\ncomputations  taining  maintain constant compe\natonal  memory complexity  inference leading 10\n fst nontrnsformer architecture toe sce  tens \nbillions  parameters rwky architect  shown  fie\n32  time complexity comparison  rwkv  different\n\nfig 32 rwkv architecture courtesy  141\n\n\u2018transfers  provided  fig 33 \u201coort 1 tah\nofer offa io\nuneaten oe\nredan ognenn  orta  3 gs\nran er ng\nic oer oe\n\nfig 23 time complexity comparison  rwky  diferent\n\u2018tansfooers denotes  sequence length \nfeatue dimension  \u00a9  mega chunk size  quadratic\ntention courtesy  4\n\n2 lowrank adaption lora lowrank adaptation \n4 popular  lghtvelght wining technique  sgaicanly\nfeduees  number  trainable parameters   based\nfon aerial insight   diffrence   fine\ntuned weights fora specialized task apd  ital retrained\ntvcighs often exhibits low niin rank\u201d  mooning \n  approximated well  low rank mate 142"}, {"page_number": 20, "sentences": "fig 4  illustration  lora reparametrizan \u2018rained   proces courtesy  42\n\n\u2018esining  lora  much faster memoryfiient \nproduces smaller mol weights 3 fe hundred mbs  \nser  sore  share one property  lowrank matrices\nshat  ean  represent   product  two smaller\n\u2018atte tis realization leads othe hypxess   dela\netween hneuned weights  inital pewained weighs \n\u2018 repesened ab  matix product  two much smaller\n\u2018abies focusing  updating  two smaller maces\nfather thn  ene original weight mat computational\n\u2018fceney   substantially improved\n\nspecifically fora pretsined weight matix wy \u20ac 4\nlora consrains  update  representing  later \n4 owsank decompesion 1p  aw  wy  ba \ndeki ac io   rank r  mdn 4 dang\ntaining 1h  fozen    receive gradknt updates\n\u2018 \u2019 conn trainable parameters tis wor\n\u2018mentioning   ty  atv  224 ae multiplied \n\u2018  input  thet respective outpat vectors  summed\ncoondinaetise h  wor thei mode forward pass\nyields f wor  aw har  baz usally \u00abrandom\nganson inializaon  wsed    ero initialization\n  20 aiv  bl  reo   beginning  tsning\n\u2018  sale aw r hy cr  eat constat  \nreparametrzaton  illacrated  figure 34\n\n wor mentioning  lora canbe applied   \nsubset  weight mates   eural network  reduce \n\u2018nombr  trainable parameters inthe transformer ahs\nure   fur weight matrices   selftention module\nly hy \u00bb m1  16 fn  mep mole mest \ntne lara  focused om adapting  anton weights\nfonly tor downstream task  reeves  milp modules\nthy ate nt tained  downsceam tasks hath  simplicity\nnd parameterecenc 5 knowledge disilation koovlodge dstilation isthe\nprocess  fearing   lager model 13 eater das \nestpecfrming model lease  proven tht  approach\n  uf even  kis used   apl dilation proach\n also refed    approach  dsl  knowledge \n\u2018ot  single model bt infact mutiple models   smaler\n\u2018one creating salle models   approach yes smaller\n\u2018model sizes  canbe used even  edge devices knowledge\naisillation  shown  fig 38 ilsrates  general setup \n raining scheme\n\nfig 35  generic knowledge dsiltion framework \nstent  teacher courtesy  l4h\n\nkowledge   transfered  dierent orn  lean\ning response stilton feature distillation snd apl dst\n\nfon response dsiltion  concerned  wih  output\nfof  teacher adel  tier  teach  sient model\ntow  exactly   lesat snarl perfor   sve \nedition athe teacher feature distllsion   ass\nfhe lat ayer  also intermediate layers se well 0 ciate \n\u2018terme representation fr te stadent mode helps \nstaller model    sinilarreeesentation   cachet\ndel\n\napi distillation isthe process  using  apl typically\n  llm provider   openal 0 tin sulle\niodele   case  llms   wed  tain  made\n  drct ouput   larger model  makes  ery\nsimilar  response distillation many concerns ae raised \n typeof dilation boca  eases whee  model self\n ot openly available  sual paid apl  exposed frend\n\u2018srs hand  ses pay foreach call ho \n\u2018ee  predictions limited  example openal pris\ntage offs apl  cxete llm  ioer ill  used \n\u2018ompete main vane im sich ease timing data\n\n4 quantization deep earing   core  st \n\u2018mathematical functions applied 10 matiees ith  specie\nprecision  model weights reducing  pression  \n\u2018weights   used  redoce  sie   mde andl\n\u2018make  faer avan example flot 32 aperations compared\n0 ine operations ae slower  proces   ella\n\u2018quantization   applied  diferent phases main ap\npoche  model quantization ean  eaeporzed 3 post\nining quintizaion  quanizationavare taining pos\ntaining quantization ie concemed  quantized ined mod\ncls  two wellknown methods dynamic  state dynamic\npostvainiag quantization computes  range  quantization\n\u2018 rutime   slower compared wo static quanization\n\u2018hare waning adds quantization citea ino taining abd\n2 quantized model  waned  optimized  taining\nprocess  approach ensures hat  end mode  \nfood performance  also doesnot need   quantized afer\nfeining\n\n  llms  used  auomented\n\n  llms  tsined   use   gener\nested outputs   variety  tasks llms   used\n\u2018ety  basic prompting however oder oexpiit\n\u2018heir fll potential orto address    shortcomings"}, {"page_number": 21, "sentences": "\u2018 ned  augment  models hough  external means\ntn  secon  fist provide  hie overview   main\nshortcoming  llms   deeper look atthe issue \nhallucination ten desert bow prompting   aug\n\u2018mentation approaches    address  tmitations\ntnt also  used  augment  capabilities  llms going\n48 far  tuning  llm   fullblown  agent  \nlity  interface   external world\n\n llm limitations\n\neis important  remember  llms  trained opi\ntoken fineuning apd aligament improves tie pee\nformance  als diferent dimensions  ther bls \nse il  important limitations  come op pricy\n   used ively    include  following\n\n   sltememory llms  thei \n\u2018aot rentember even   sent 16   \nfrevious prompt tati  importatlimiation \n ofthe uses cases  requ  form  sat\n\n   stochastifprobsbilisie  send  \nromp oan llm several mes   hcy  et\nuileret responses   ae parameter fn panicular  temperature  limit  varity\n4  eesponse   inherent property  dee\naining mat  create sues\n\n  lle information   ther  \nfave access  extemal data  llm    \n\u2018ot even know abou  cute time  ds  \nfot  acess oa infomation   ot present\nins usning st\n\n ae generally  lange means  many\nowly gpu machines ate needed  taining serving cass largest models ave pooe\nsuas partially  temas  latency hallucinate llms    4 notion \nuth    usually  tained mmx\nfof good  bad content   produce \nplasble  unt answers\n\nwile  previous limitations   tecome important\n  appitation ts worth  us  dive abit imo \ntas one hallcinations since thas gathered lot  intrest\n\u2018  pst  months  thas aso sparked many  te\nprompt approaches  llm augmentation methods  il\nther deserts\n\nhatucination   realm  large language mods\nllms  phenomenon  \u201challucinations\u201d bas garnered\n\u2018seniticantatention defined   iterate notably  \n\u2018ervey  hallocinaon  natral langhage generation\npaper 145 fallin   llm  characterized 3s\n generation  content   nonsense fathfl\n  provided source\u201d  terminology althvgh rooted \npsychological parlance  ben appropriated within  el\nsf ati ineligene hallucinations  llms   broadly categorized ito\nwo 1s\n\n1 intrnsle hallucinations  dsety confit \n source materi inoducing fatal inacuracies\n logical inconsistencies\n\n2 extrinsic hallucinations thess  aoe con\niting unverifiable   source eacom\nsssing speculative  unconfinable element \u201c definition  source\u201d  llm context varies  \ntask  dalogoe hase tasks ters  world knowle\u2019 \u2018shores intext summariction je pertains tothe  ex\nyell  dstnetion plays  cructl eol  evaluating \ninverting hallucinations impact  hllacinations \nbighly contextdependene  instance  erative endeavors\nike poem wring hallcinaions might  deemed acceptable\n\nms tained  diverse datasets inclng  internet\nbooks wikipedia generate text based  probable\n\u2018models without  inherent understanding  tah  falsity\nrecent advancements lite instruct tuning  reinforcement\nleaming  human feedback rlhf  atempted\nsteer lems towards  fatal outpts   fundamental\nfpobubiistic nate  ts inherent katations emain \nfecent study sources  hallucination  large language\n\u2018modes  terence tasks\u201d 146 highlights wo key aspects\nontibutng  ballcinations  llm  veracity prot \n ative frequency heuristic underscoring  completes\ninherent  llm waning  output generation effective automated measurement  hallucinations \nllm requires combination  statistical  modelbased\n\nsuatsica metres\n\n\u2018\u00a9 meares ke rouge 147  bleu 148  com\nton fr asessng text smi focusing  ntsc\nfallainations\n\n\u2018\u00a9 advanced meres suchas parent 149 parent\niso  knowledge fi 151 ae utilized \nsinictued knowledge sources avalsle etree  efective  kinison  eating\nsymactic  semantic nuances\n\nmodelbased metrics\n\n4 tetased metrics utlize information extrction\ndels  simplify knowledge ito relational types\n\u2018hen compare    source\n\n qatased metrics assess  overlap  gen\nfated content   source rough  queson\nshowering framework se 152 4 nlebased metres use naural language inference\ndbatcets  evaluate  tthfulness  8 generted\nhypothesis based   given premise see 183 faithfulness classification metrics offer refined\nsessment  creating uskespecific datasets tor \n\u2018anced evaluation see 158\n\ndespite advances  sutomatedmtres human judgment\nremains  vital piece ypiclly imvaives two methodologies"}, {"page_number": 22, "sentences": "mow lis ae used  augmented\n\nsprompt \u2014vacems seme\n\nfig 36  llms  used  augmented\n\n1 scoring human etatuators rate  level  halle\nraion within  predefined scale\n\n2 comparative analysis ealuatos compare gene\nsted conten aginst baseline  rounderth rele\nees adding  essential layer  subjective asses\n\nfactscre 155   recent example  \u00abmute  canbe\nwed   human  nodstbused eralustion metic\n\u2018weak ab llm generation ino \u201catomic tat  bnal score\n computed asthe sum ofthe accuracy ofeach atomic act\nving    equal weight accuracy  abiary number\nhat simply sales wheter  atom fact supported  \nsource autos implement diferent automation strategies\n\u2018hat use llm  estimate  metic\n\nfinally mitigating hallucinations  llms sa mulated\nctullengevequiing talloed sttepes  suit varius applicae\n\u2018ons thowe include\n\n produet design  user ineracion strategies \n use case gedign suctring  input \nproviding mechanisms  user feedback\n\n dats management  continous improvement\n\n\u2018maintaining  analyzing  tacking set ofhllcina\ntion essential  cngoing model inprovemest \u2018prompt engineering  metaprompt design many\nfof  advanced prompt tshnigues descebed  1v8\n  reuioval augmented generation divcty \n\u2018tes hallucination ike\n\n model sleton  configuration  hallcination\nmiigaion exemple lager models wilh lower\ntemperature setiags usually perform beter alo\ntectiguer sich  rlhf oe domainsepcitic sine\n\u2018ning  mitigate hallucination sks\n\n1b using llms prompt design  engincering\n\n prompt  generative al models   textual pat\nprovided  uses  guide  model output could\n\u201cge fom simple questions wo detaled descriptions  specie\ntasks prompts general consist  insrctons questions\ninput data examples practice w elicit  deste\nresponse    mods  prom must conan either\ninsrutions  questions ith  elements  opin\nadvanced prompts involve  complet sructtes nich 3"}, {"page_number": 23, "sentences": "\u201cei  thought prompting mode guided 19\nfollow  topical reasoning process 1 ative   answer\n\nprompt engineering   rapidly evolving discipline \nshapes  iterations  outputs  llms  eer gen\nratte  models  exsence  prompt enginecring les\n\u2018rafting  apimal prompt  achieve  specie goal ith\n4 generative model proces sot   insroting\n mode bats involves  understanding   \nipabiics  limitations   context thin  \noperates\n\nprompt cagnceriag transcends  mere construction \nromps  requies 2 bend  domain knowledge understand\ning   al model  methodical approach  aloe\n\u2018prompts  diferent contexts  might involve erating\nfemplates    programmatically modified based  2\nven dataset  context  example generating personalized\nfesponses based  user data might use  template  \nfynacally fled  relevant usr information\n\nfanhermare prompt engineering isan iterative  ex\noratory process akin 40 talon machine leaming poe\nces  ah model evaluation  hyperparameter unin \n\u2018pid growth ofthis field suggests  potential  revolute\ntin aapects  machine lesming moving beyond atonal\n\u2018methods hike feature  actetue engincering \u2018 wadivonal engineering pracices suchas version cou\n\u2018wo  repression testing aced   adaped   new\nparadigm jst ike  weee adapted   machine learing\nspproacs 61\n\n  following paragraphs  deal  ofthe mst\nimeresting  popular prompt engineering approaches 1 chain  though cod  chain  thowght co\ntechnigu inially deserbed inthe paper \u201cchait thought\n\u2018prompting elicits reasoning  lage language models 34\n goople researchers eepesents 2 piveal advancement \npompt engineering  lavge language models llms\n\u2018 apeoach binges   understanding  llm roicient  token prediction ae nt inherenly designed \nexplicit reasoning cot addses ths  guiding  model\nahrough esental reasoning steps\n\nco  based  making  implicit reasoning proces \n\nllms explicit  outlining  steps required fr feasoning model  directed closer toa logical  reasoned outa \u2018specially  scenarios demanding tore  spl infora\n retical  paler recogni\n\ncot prompting maifes ia two primary forms\n\n1 zeroshot co  form ivolves istrict \nlem  \u201cthink step  sep\u201d prompting   de\n\u2018onan  problem  agate  sage \n\u2018easing 2 manual cat   complex variant requires\nproviding sepbysep tenoning examples 13 ten\nplats fr  model  yieking  effective\nfeats  poses challenges  salabity  ine\n\nmansal cai   elective thin zer0shot however\nte effectiveness ofthis eampleased cot depends  \ncoice  diverse examples  constructing prompts \n\n examples  step  stp reasoning  hand  bard \nfoe prone tat   automatic co 157 comes \nply\n\n2 tee  thought te  tree  thought tot\nlus\u00e9 prompting technique  inspired bythe concspt \n\u2018considering various ltemative solutions  thogght processes\nfore commerging    plausible one tot  ised\nfon  idea  branching   multiple \u201cthought tes\u201d\n ach branch represents diferent tine  reasoning\n\u201c method allows  llm  explore rious possiblities\nsin hypotheses mich ike harman cognitive proestes \n\u2018ple senarior ae considered  determining  \nmikey one\n\naccel aspect  tot   evaluation  thse reaoning\npaths   llm generates different ranches  thought ich  aseved fore validiy  relevance tothe query \u2018 proces involves realtime analysis  comparison \n beanches leading wo  slaction ofthe  coherent abd\ntopical eutome\n\ntot  particulary useful  complex problemsoving\nscenarios  single line  reasoning might nt sie\nreals llms  mimic 4  hamanike problem song\napproach considering  range  possibiliies  arising\nfata conclusion technique enhances  model ability\n handle ambiguity complex abd nuanced ask making \n4 valle tool n avaned  applications\n\n5 selconssteney sel\u00e9consiseney 1591 wilizes \ncensemblebased method wheve  llm  prompted  rew\nrate multiple responses    query constancy\nsong thee responses serves   indleatrof thei accuracy\nsod reibly\n\n\u201c self consistency approach  grounded   piaciple\n\u2018har   llm generates multiple similar responses  \n prompt ti moe likely thi  response  accurate\n\u2018 metho involves asking  llm wo tackle  query mule\ntile times tne analyzing  response  consistency\n\u201c technique  especialy sel  scenarios  factual\naccuracy  precision  paramount\n\n\u201c consistency  responses cane measured using\n\u2018ous methods one common approach sw analyze  overlap\n  content ofthe responses  mato may include\n\u2018comparing  sete similarity  responses  employing\n\u2018nore sophisicaled techniques lke berescores  ham\n\u2018overlaps measires help  quantifying  level \nsgreement anon  responses gectued bythe llm seleconsstency signtcant applications  felis\n\u2018  veracity  information fe crteal wi parila\ntelevnt  scearioe like facchocking  ensuring \naccuracy  information provided   modes  essen\n employing  tecmigue prompt engineers  enhance\n msworthiness  llms making  tore feible \n\u2018asks  require high levels  fatal accuracy\n\n4 reflection reeton 160 wolves prompting llms\n assess  potetlly reise   outputs based \nreasoning   comecness  coherence   fe\nsponses concep  reflection centes onthe ability \nttlmes  engage inform  selevaution arer genesting"}, {"page_number": 24, "sentences": "inal response  model  prompted  reflect  ts\n\u2018 outpat considering fates ike actual accuse logical\n\u2018sonsstency  relevance inospective process cat lead\n  generation  revised  improved respons\n\n key aspect  reflection   llm capacity \nselfeing evaltting  nial response  model con\n\u2018entity potenttalenorso teas  iranement erative\npocess  generation reflection  eevision enables  llm elie  output enhancing  overall quality  eabiity\n  responses expert prompring\u00bb exper prompting 161 enbances \ncapabilites  large language models llms  simulating\n eesponses  exper  vaous hels metho involves\nomptng  lum  assume  cole ofan expert  \nspend accordingly providing highquality formed answers\n\u2018ax key suategy within exper prompaing   multexpet\napproach llm  prompted  consier responses fom\n\u2018ple expert perspectives   thea synthesized\nforma comprehensive abd wellrounded answer tech\nigue   enhances  depth   response tat also\nincorporates range  viewpoints electing 4  holsic\nunderstanding ofthe subject mater \u20186 chains chains refer othe method  linking mitpte\n\u2018components  sequence  handle complex asks  large\nlanguage models llms approach involves cesing 8\nseries  intereonnected steps  processes cach contributing\n  final outcome concept  chains  based \n idea  contracting  workflow  diferent stages\n\u2018 components  sequently arranged componest \nchain performs  speci funtion te eutpt  ene\nservers  input tor  next endend arrangement\nstage canbe tlored fo handle 3 specie aspect   tsk\nchains  vary  complexity  sructire depending \n requiements prompihsiner chaining large lan\nfquage model proms though visual programming 162\n authors pot  deserie  min challenges  ssiing\n\u2018thins bu alo describe vst  0 support  tsk\n\n7 ras rails  advanced prom engineering refer 10\n\u20184 method  guiding  contoling  cup  large\nlanguage models llms  predefined rules  em\nplates sproach  designed  ensure   mode\nfesponses adhere  certain standards  ete enhancing \nfelevance safety accuracy ofthe output concept \nalls invaves seating  framework oa set  guidelines\nthatthe llm must fllow  genorating responses uidlines  typically defined using  modeling language \nfemplaes known  canonical fam  standardize \n\u2018way natural language seatences ae stuctured  delivered\n\nrails   designed  varios pupoaes depending \n specie ness   application\n\n topleal ralls ensue   llm sticks 1\npattcuar topic  domain\n\n factchecking rails aimed  minimizing  gen\ntration  false  misleading information\n\n iuilbreaking rai prevent  llm  generating\nfesponses tht attempt  bypass   operational\nonsraints  guidelines\n\nawomatie prompt bugineering ape automatic\n\u2018prompt engincting ape 163 focuses  automating \nimocess  prompt creation  large language models\nlms ape seeks  steamline  optimize  prompt\n\u2018design process leveraging  capabilities  llms \n generate  evaluate prompts ape involves using llms\n  scleveferetial manner   model ix employed\n generate score  refine prompss reeursive use \ntelm enables  creation  highquality prompts wat \n\u2018 likely  clit  desired response  outcome \u2018 methodology  ape canbe broken   several\ney tps\n\n prompt generation  llm generates  range \npotential prompts based   given task  objective \u2018 prompt scoring  generated prompt 8 \nevaluated  ity effectiveness oflen tng eter\nike cary spect  ikelihood  ein \ndesiced expanse refiwement  heration based   evs\n\u2018hons prompt   refined  ete upon frtber\nfenbanting tir quality  effectiveness\n\n\u00a9 augmenting llms  external knowlege\u00bb rag\n\none   main limitations  peained llms  dei\nlack  dte knowledge  access  private  use\ncsepecite formation    retrieval augmented\nsnertion rag comes ino  pitute 16s rag ise\ntated  figue 37 involves exractlng  query fom  inpat\nprompt  using  query  revive felevant information\n  extemal knowledge source ea search engine 02\nknowledge graph see figure 38 elvan information \n\u2018 added   orginal prompt  fed   lem inorder\nforthe model  generate  nal response rag system\nincludes tee imporant components revival generation\n\u2018augmentation 168\n\n4 ragavare prompting techniques   \nimporance  rag  build advanced llm systems several\nragaware prompting techniques  een developed te\ncet one sch techniques forwardlooking active reveval\n\u201caugmented generation flare\n\nforwantlookiag active retieval augmented genestion\nflare 165 enhances  capabilites  large language\n\u2018models llms  iteratively combining prediction  \nformation rerival flare represents  evolution  \n  revievalaugmened generation aimed  improving \naccuracy  rlevance  elm responses flare involves  iterative process   llm acsively predicts upcoming content  uses  predictions\n8 quenes  revive relevant information tis method con\n\u2018eats  tadtona evievalaugmented modes tha tpically\n\u2018etieve information   thea proceed wih generation flare  process  dynamic  ongoing throughout \nasneration phase flare  sentence  segment genet\nsted   lem  evaluated  condence ifthe conence\nlevels   ertain threshold  model uses  generated\nomtent 9 query  retrieve relevant information  \n used regenerate  rotine  sentence  erative"}, {"page_number": 25, "sentences": "fig 37  example  synthesizing rag  llms  question answering aplication 16\n\n38  ise example  synthesizing  kg \nrethever  llm 167\n\nrocess ensures   pat ofthe response  informed \n  relevant  current information avalsble\n\n  detils  rag framework adits relevant works\n refer  readers   survey  revival augmented\nseneratons 165 using external tools\n\nretiving information rom  external knowledge source\n deserted shove fen one   potential ways  augment\n lem generally  llm  access  number\nff exeral tools eg apl wo  sevice  algment ts\ntunetonaliy segads rag   seen a6  specie\ninstanceof  bade eategory ofthe  elle yoo\n\n\u201ctols   context ae external functions  services hat\nlllme  uli tole extend  range  tasks \nllm  perform rom bai information reel  complex\ninteractions  external ditabases  aps\n\n  paper \u201ctoolformer language models  teach\n\u201c \u00a9 use tools\u201d 169 authors go beyond simple\ntool usage  training  llm  decide   1 use\n\u2018shen  even shat parimeters  apl needs tools nce\n\u2018vo aferet search engines oa aleulator te  following\n\nexamples  llm decides 19 call  extra qua \n1 calculi   wikipedia search engine  recent\n\u2018esearcher  berkley  tained  new llm called geils\n67  beats gpt atthe use  apis specie bu quite\ngeneral tol\n\n4 toolaware prompting teclniques sisal 1 \n\u2018 described  rag several toolawate peomping ap\npouches   developed  make usage  tols mete\nscalable popular techniques   called automatic mul\nstep reasoning  tooluse art\n\nautomatic multstep reasoning apd ase art 170\n\u2018  prompt engineering technique  combines automated\nchain  thought prompting   use  exeral tools\nart represents  gonvergence  multiple prompt engineering\nstrategies enhancing te ability  large language models\nllms  andle complex tasks  require  reasoning\nsn interaction  externa data sources  tools\n\nart involves  systematic approach  given tsk\n input  sytem fst iets similar asks   task\nia  tasks ae  aed a8 example nthe prompt\nguiding  llm    approach  execute  cutest\nask  method  particularly effective  tasks require \ncombination internal easing  exer dla processing\n\ne llm agens\n\n\u201ctheda  al agents hasbeen wellexploed inthe history\nfof al  agent  typically  autonomous entity  \nperceive  enviconment using  sensors make  judgment\nbased   stae  eurenly   accordingly  based \n\u2018 ations  sre avable \n\n  context  llms agent refers   system based\nfon  specialized snstanistion   \u2018otigmented llm thst\n capable  performing spectc tasks autonomously gent  designed  interact  ses  environmen\n\u2018ake decisions based   input abd  intended geal \n iteration agents  based  llms equiped"}, {"page_number": 26, "sentences": "aly  acess  use tos   make decisions based \n given inpt designed  handle asks  require\n degice  autonomy  decision making typically beyond\nsimple response generation\n\n\u201c funcinalies  generic llmcased agent include\n\ntool access  uiization agents ive  capil\nity  acess external ool  services   wie\n resources effectively 10 accomplish tasks\n\n\u2018\u00a9 devsion making  ean make decisions based \n input context   tools avalible  \nfen employing complex reasoning processes asan example  llm   access oa funtion oe\n apd  se weather apl  atewer  question elated\n  weather ofthe specie place  ther words  use\n\u2018apis  solve problems furtermove  hat lem  access\n\u2018  api  allows  make purchases  purchasing agent\n  bul \u00a9  ly  capabilites  ead information\n  extemal word bt also act omit 171 fig 40 shows anther example  llmbased agents \nconvessionl information seeking 36 whore  llc \nsugmented   set  plyganiplay modules incloding\nworking memory  tacks  dog sats 4 poley \n\u2018makes ap exceution pla   tak  selects next stem\ntin  ation executor hat performs  acon selected \n policy consolidating evidence fom extra knowledge\nff prompting  llm io generate responses  aity\n\u2018hat accesses  alignment ofthe llm responses  user\nexpectations  speci business requirements  generate\nfeedback  improve agent performance\n\n  details  llmhased al agents se recent survey\n\n2p 73 178\n\n4 prompt engineering tceliques  agent like\nrag  tools prompt engincering techniques  speci\nically address  needs  llmcbased agents  \neveloped thre  examples  reasoning without ob\nservaton rewoo reason  act reach  dislog\nenabled resolving agents der\n\nreasoning without observation rewoo 178 aims 10\ndecouple reasoning  dec obsertations rewoo operates\nbby enabling llm wo formulate compeeensive easing plans\n metpans witout immediate foliane  external data\n\u2018 tools  approach allows te agent  este aac\need framework  fesoning    exeuted  \nfocesaty data  observations ate alll ia rewoo telm intially develops  plan asevies  steps  oines\n \u00a9 apyeoach  solve given problem mee\nplaning pase  caval ae ste  stage forthe agent \nproce information   becomes avaiable te exciton\nhase  invlves integrating acta data \u00a2abservations \n prespecified plan lang  coherent  contextually\nfclevant responses rcwoo offer significant advantages \ntems  token efciency  robusiess  tol faire tt\nfmibles llms  handle ticks  immediate acess 10\nxterm data   aisle relying instead   well structured reasoning famework method  paiularly\n\u2018xantageos  scenarios whore dts retival scot lw\n\n\u2018 uncertain allowing  llmcbased agent  maintain high\nteel  peefomance  slay\n\nreason  act reaci76 prompts llm  generate\n\u2018nt  verbal reasoning\u2019  also sctonable step thas\nuhancing  model dysamic problemselving capabilities\nreaet  grounded   principle  iterating teasoning\n\u2018 action   approach  llm  prompted  alterste\n\u2018 generating reasoning traces explanations  aking\nactions steps  commands   interleaved manpee approach allows te model dynamically reason abouts prob\ntem  propose snd take concrete actions simultancoly dislogenaled resolving agents dera 177 ae spe\nciaized  agents   engage  logue resolve queries\nsind make decisions based  interactive exchanges dera\nie developed based   idea  utilizing multiple agents\n\u2018within  log context cach  specie rles  fnctons\n\u201c agents cin inside researchers ho gather  analyze\ninformation  decides  make fina jadements based\n\u2018  information provided  division  role allows \n2 welhorginized  efcint approach  problemsolving\nsand decisionmaking dera  paricularly advantageous \nscenarios requiring complex decisionmaking  preblem\nsolving sachs thse  medical iaptics  custome sr\nvice collaborative ad interactive nature  dera agents\nallows dhe  handle iteate queries  teel  depth\nsd nuance  sngleagensjsens might sige th\nmorcover thi approach ligne ell  human decison\n\u2018making procesce making ai rewoning  rlaable sn\nimisinocy\n\npopular datasets  llms\n\nlinge language models exhibit promising accomplish\n\u2018ments   main question  aes   effectively\n function    performance   assesed \nspecific tasks  applications\n\n\u201c evafution  llms poses puticlar challenges due\n  evolving landscape   applications orignal\ninvent sohn developing llms wast hoos  performance\n nlp tasks sic stanton semmarzationeston\nsnswering  soon 178 however tis evident today\n thete models  finding uty scroxs diverse domaine\nincluding cde generation  france moreover  va\ntation  lems encompasses several ential considerations\nsach ss faimess  han lactcheckng  essoning section  cutive  commonly ted benchmarks \nsessing llms benchmarks ae categorized based \n\u2018ining  evaluating  llm capable\n\ntasks\n\n datasets  basie model inginderstandinegeneraton\n\n\u2018 section provides  overview ofthe benchmarks \natsets ste  evalst  hase ables  llms\n\nlanguage\n\n4 natural questions 179  4 qa dataset dha consists\n\u2018 real anonymized aggregted queries sabmited \n google serch engine  questions anna\njs presemed wath  question along   wikipedia\npode fom  top 3 search resis annoates \ntong answer pica paragraph anda shot answer"}, {"page_number": 27, "sentences": "b hoesingset\n\ndumas controter\n\nce ae \u2014\n1\n\nestate ff \u2014\n\nsoa \u2014\n\nfig 40  llmcbased agent  conversational information\nseeking courtesy  136\n\none   ens  present   page  marks\nfull   logshrt answer  presen\n\nmmu 180  imtended 10 evaluate  knowl\nge gained  zeroshot  sbot senation fmeans  mmlu aseses   general knowl\nge  problemsolving ability   model coves\n\u2018sp subjects  stem humanities social science\n  areas  beochmatk varies  complexity\nfanging fom elementary  advanced peotessiona\n wor meationing thatthe main comtibuion \n dataset   mulitask language understanding\nquestion answering apd arithmetic reasoning mpp 181 sands  mostly basic python prob\njems  provides  benchmark  evaluating \nperformance  models designed  code generation\n benchmark encompasses 974 short python pro\ngrams including\u2019 wide range  topics including\nfundamental programming concepts  standard \nbrary sige    challenge comprises 3\n\ntask description \u00abcade soltion  thre automated\n\nhumanbva 182  4 dataset  code generation\ntask tis dataset consi  168 handcrafted pro\n\u2018ramming challenges challenge  sccompanied\nbya funtion signature desing code body  al\nne unt ex  mala intuition behind developing\n dataset  guarattoe  exclusion   contents\n\u2018tom raining datasets  code generation models\n\napps 183  designed  code generation task\nfocusing   python programming language apps date contigs cllection  2114 python\nroprams program inthe dataset   average\nts lines  python code addionll apps oes\nsecess 10  repository  10000 unique programing\nveces   textbased problem descriptions \u2018 tinal aspect wo highligh ht   includes es\n\n\u2018whkisql 184 x rated fr coe generation ask ad\n\u2018thas 87726 eaetlly labeled pais  sql queries\ntnd corresponding natural language questions \nwikipedia tables sql queries comprise thee\nsubset test ses 17281 examples development\n9145 example  tring 1297 examples\n\ntrlvlnqa 185  designed  qa tsk tis\ndauset comprises move  650000 question\nnswervidence tips ae 05000 queston\nanswer pais inthis dataset cach authored  via en\nthusists  supported  average  six indepen\n\u2018ently sourced evidence documents  document\nfe automaticaly acquired frm wikipedia  broader\n\u2018web search ress  dataset  eatgorzed io\n\u2018vo segments intoding   authentic answers\n wikipedia  weh domains nd erie sts\n\u2018mod  accuse answered gestions slong ith\n asotated documents fom hot wikipedia sh"}, {"page_number": 28, "sentences": "fig 41 dataset applications\n\nrace 186 suits  ceading comprehension tisk\n\u2018 dataset  based  english tests competed \nchinese students  mide schol snd high choo\n\u2018god 12 conttins roughly 25000 texts\n 100000 questions sgorously prepared  human\nspecialists eimarily english instructors datas\n\u2018ontains \u00a9 wide cange  subjects   purpose\nfully chosen  astee sens comprehension \nreasoning abilities  date  ava  thee\nsuhgroups race  raceh  race race\n refer tothe middie school examinations whereas\nraceh denotes  high stool ests finally ract\n\nsth syrubesis  race  raceh squad 187 stands  \u201cstanford question ans\ning dataset\u201d  isa crowdsoured reading compr\nhension dataset based  wikipedia articles tt \napproximately 100000\u201d questonanswer pais con\nowed meve  s01 arcles  answers \n questions  typically text fragments  spans\ntaken   comesponding reading passages \n\u2018questions may  unanswerable   cases \u2018hata  dived nto thre set  0 tuning st\n110 development st   10 hen test se"}, {"page_number": 29, "sentences": "\u00b0  oy  \u201cfs eee se eae 2 ra\n  oe pee pi le\n\noof datasets\n\nfig 42 datasets licensed  different licenses\n\n boolq 188   yesfoo questonanswering dataset\n  goal  weading comprehension ask booiq\nincludes 15042 examples example isa tiple\n includes \u00abquestion relevant paragraph \n\nsolution althoggh  main intuition behind\n dataset   reading comprehension   \n\n\u2018red  resoning nara language inference \n\n\u2018gestion answering task\n\n\u00a9 mulgirc 189  another dataset  fits reading\ncomprehension task mulrc contains brief par\nrine  well ae mmltisentencs questions tht \nfe answered using  information inthe paragraph\n\u2018 paragraphs inthis dataset come froma varity\n sources nelading news ftonbislorial texts\n\u2018akipedia artes discussions  society  \nelementary school science textbooks al 911 42\nors guest  many respons coe \nfone   ot   corel answering \n\u201cquestions requires reasoning actos several sentence\n\u2018muuirc dataset encompasses around 6000 muli\nsentence questions gathered   800 paragraphs\n average cachquestion les  wo valid\nfnawer alternatives   4 ttl  bie\n\ndatasets  emerge icl reasoning cop insruton\nfollwing\n\n\u201c seston centers om  henchmarks  dates em\n\nployed  erafuate  emergent bites  llm\n\ngsmsk 190  designed  evaluate  model\nbility  multtep mathematical reasoning gsm3k\ninclides 85k linguistically diverse grade schol math\n\u2018word problems wten  humans dataset  pit\nito two sets  training set  tac problems\nfand est set \u2018 ik problems  problems\nneed 2 t0 8 step   solved solutions mainly\nfe  series  elementary caleulations wing basi\nhnetic operations\n\nmath 191 enables  assess  well models \nsolve math problems math dataset hast 12 500\nproblems  high school math competitions droblem inthe dataset   stepstep solution abd\nfinal answer enclosed  abo problems cover\n8 wide range  topics  hive diferent levels \nompleiy ther ae seven sj  toa fae\n\u2018  difiuty   problem  ated based\nfon  aops standards onsale fom 10 9\n7 shows  easiest problems ina subject wile\nrepresents   ical  terms  formating\n problems  soltions  presented using latex nd  asympiote sector graphs language\n\nhetlaswag 192  designed w assess commonsense\nreasoning  llms  benchmark includes 70000\niutplechoie questions  question derived\nrom oe  ho domains actiitynet ce wikio\nnd presents four answer choices regading \u2018ight happenin  following station core\nsnswer provides  actu statement describing"}, {"page_number": 30, "sentences": "upcoming event bur  tee wong answers ate\n\u2018ested  confuse machine\n\nabe reasoning challenge arc 193  used\n commonsense reasoning benchmark encom\npasses 7747 science examination questions \nquestions   english      set\nip int muliplhoice format questions \n divided  two groups  challenge set \n2500 ificult questions   easy set  5197\n\u2018questions collection hss also een peivided\nite tran development  test subsets\n\npiqa 1193  intended  evaluate  language\nrepresentations  thei knowledge  physical com\nimonsense thn dataset  focus   everyday\nfitations witha preference  uncorimon sauces \u2018 conta tsk ie muliplechoice question answer\ning   question g  provided along ith 0\npotent solutions 12 best soltion \nhesen  whether medel  3 human \u2018question  one   solutions   core\n\nsiqa 195 provides  framework fr evaluating mod\n\u2018ably  commonsense ressoning  socal\nstations siqa dataset  3000 mulple choice\nquestions designed fo assess emotional  social\ninteligence  everyday circumstances dataset\ncovers  wide varity  socal scenarios siqa potetal answer   mixture  humanselected\nresponses  machinegenerated ones hat  \n\u2018tered though adversarial processes\n\nopenblookqa obqa 196   new kind \n\u2018qestiomanswering dataset  answering te qe\ntions requires additonal common  commonsense\nknowledge  contained inthe book  rich text\n\u2018omprehenson ite incides around 6000\n\u2018mulplechoice questions question fs lined 10\nfe core fact ell   akitonal collection\nff  bn fact questions  developed\nusing 4 multistage comdsourcing  expe er \u2018ng procedure openbook qa questions  iit\n\ntop reasoning  limited\n\n\u2018truthfulqa 197  designed specially v9 eval\nuate  teuhfulaess  language models  gen\nfating answers  questions dataset includes\ns17 questions ween  authors fom 38 diferent\ntego including health la finance  polis \u2018 questions ae purposefully designed  chal\njenge human responders  may contin common\nmisunderstandings  lead  incoeect answers\n\noptamil bench 103   comprehensive bench\n\u2018mark  instiction metleaming covers 2000\nnlp tasks fom 8 existing benchmarks optml beach consists  ataning set  179  examples\ndev set  t4sk samples  atest et  321k\nsamples\n\n\u00a9 dataset  augmented using external nedetools\n\n\u2018 section formes  datasets designed   ap\n\u2018mented abies  lim\n\n\u2018\u00a9 hotporqa 198  designed  cover  diverse \nexplainable guestionanswering dataset  neces\ntates multihop reasoning datasets derived \n english wikipedia consist  oughly 113000\nquestions question im  dataset comes \nvo paragraphs called gold paragraphs  two\nwikiped aricles alo  ia   sentences\n  puragrop  growdworkers  picked \nimportant  answering ihe question\n\n\u2018\u00a9 toolqa 199   question answering benchmark\n evalte llms ability t0 ase external tos \nanswering gestions gptstole serves   instructional dataset gene\nated  instructing advanced teachers   chat\ngp wth instrctions conditioned  visual conten\n tool descriptions process results  \nseneaton  instructions relat   use  1s\n  thee versions   dataset fist\nversion comprises 7000 instruction following data\npoints wiz  nestne  gptstoos model \next version consists  inal cleaned insution\ndata used  validation covering instructions relate\n te tools   fist version  last version leaned insrution dala use  testing  cade\ninattions related yo ame tol tha te ot present\n  fiat vee vi prominent llms\u2019 performance \nbenchmarks\n\n  section  fist provide  overview   \npopular metrics usd  evaluating  performance  llms\nliner different scenarios   look atthe perfomince\n prominent lage language models   ofthe popular\n\u2018tacts  bonehimarks popular metrics  evanoting llms\n\nevaluating  performance  generative language modes\ndepends onthe underving task   going   tne fo \u2018tasks   mostly  slecung  choice   given\n\u2018ones uch a5 sentiment analysis seen  simple \nlassifeation  ei performance   evalated using\n\u2018issifeation merce metrics   accuracy precision\nfeel fi te  applicable   case  also ipertant 1\n\u2018ot thatthe answers generated   model  specif asks\n\u2018ich 3 miltchoice question answering ar lays ether te\n fase   answer ie nti st  opis san  sen\nse a8 \n\nhowever  tasks   purely opennded text gene\ntion cannot beers   way afr categorization\ndifferent metrics  required forthe specie purpose  \ncalation code generation  4 sey diferent ease  open\n\u2018ned generative evalations generated code must pass\n test ste bat   oer band  alo important\n understand   model  capable  generating diferent"}, {"page_number": 31, "sentences": "table il llm datasets overview\n\ntin le te pity fin\n\u2018case works   manner  given  problem different em \u00b0\nsiesta  cat\n\n\u2018solutions   respecte e numberof   coacet\nquaton provides  nal value\n\nposskr bf\n\nexact mutch em  another metric   mostly con\ncerned wth exact matches irom predefined answers \u2018ounts 2 predictions correct   enatly matcher one \n\u2018nore thin oe desired reference tex taken  token ses itean   sre  accuracy   equation 5 shows\n\u2018 mathematical deinton    ft umber  corect\nsnswers  nis  tol ner  questions 202 altemative  ft score 203 heqq represents  precision\n inividual questions whesio  ane deemed eoeect\nifthe model fl score surpasses  average human fl score\nlikewise heq denotes  precision ofeach dialog\nemed accurate  al questions within  dale mest\n ete  heq 182\n\nevaluation   generative tasks   machine tans\nlation  based  metrics   rouge  bleu \nscores work well  thee iss reference txt  ground\nteu sch  asltion   hypothesis  generated\n  generative model n  cise  llm scores\nfate mowed  cass   gosl 0 delet \nsilay   answer  ground tnth   computation\n\u2018manner ina computation manner  meant  nothing mere\n ngrams would  sed however mets sich 36 bert\n\u2018score  also bood fr  cases    also hewily"}, {"page_number": 32, "sentences": "\u2018table il llm categories  respective definitions\n\n\u2018table iv ditloren llm categorization\n\ndam eet \u201csient tos\n\nsg bes\n aah\n\noe eee\n\ne\n\nsioa \u2014 ers\n\nroncous  another mods used  judge sil ven\ntoday evaluating purely generated cont  haa \n\u20180 completely titing mete   tound metics ate either\nooking  sinplie fetes  ax ngram skipgram\nfe  hey ate models  unknown accuracy  pecaenest\neos\n\ngenerative evaluation metric ate also anther typeof ele\nuation metic  lem  use note llm fr evalting\n anster however depending onthe task isl evaluation\nfin possble  thin way another dependency\nhat maker generate evaluation errnrone ts reliance \n pomp ise ragas  one ofthe pol examples tat\nincorporate  usage  generative evaluation\n\nvarious benchmarks  leaderboards   proposed\n address   challenging question   worl \ntange ianesage mods  one  beter however \n4 simple answer ean adress  question answer je\nends  various aspect  lage language models section v\nshows  categorical presentation  diferent tasks  \n\u2018ost important dass   category wil follow \n\u2018sime categorization  provide comparison based oh \nategory  providing comparison foreach category \n\u2018sll provide  broad overview  repel performance \nserine  repored performance matric  dillerent task evaluating diferent llms canbe seen also  diferent\nperspectives  example  llm  2 dastcally fewer\n\u2018umber  parameters   completly comparable 10 one\n\u2018 large numberof parameters perspective \u2018ill ctegorize llms  four categories  well small ess\n\u2018han  equal billion parameter medium  land\ntohilion large  10 apd 10 lion   large\nvor  100 bilion another clasifction forthe llms\n\n use  tet primary use ease consider  llm 19\n\u2018either foundation model pretrained language mods \n\u20180 instuction finetuning  chat baetuning tasteuetion\n\u2018model prevained language model  oalyusseuction fine\nfuning chat model peti language mode \nsinsation  chat bnetuang apart tom   eatego\n\u2018ization deserted anther category  required  distinguish\netween orignal models  tuned ones orginal models \n  ave  elessed sea foundation modelo fine\ntuned one tuned mols    grsped  orignal\niodel  tuned   diferent datasets  even diferent\ntaining approacies iti ls good  note  original models\nae ustally foundation models    finetuned \nspecific datasets  even diferent approaches availability \n model weighs regardless ofthe cease  another ategory\nnour classifieston models   ther weights public\u201d\n\u2018ailable even theugh request ae noted  publie models\nsehile others ae noted ax private table il shows   \n\u2018etniions  abbreviations used   vest   acl\nfrgre 4 illustrate  vl\n\naccording tothe provided categorizations  ean eateg0\nsize  label  notable llm  shown  table tv san\n\u2018essen fom  ale moves etegrized   large \nako unavailable  wel\n\n1b llms\u2019 peformance om diterent tasks\n\nccommonsense reasoning  ne ofthe important capbili\nties  model  cba tis capability denotes  ability\nfof  model  tse prior knowledge  combination ith\nreasoning sil inthe ease  hellaswvag  example finding\n continvation  text challenging   given text\n\u2018omtine pata part   story whe  given choses\n8 continuation  tecky  select  without  prior"}, {"page_number": 33, "sentences": "fig 43 llm categorizations knowledge abou  world  ot posible specific kind\n reasoning deserves high atenon    elated \nlullaing previous knowledge  open textsesried scenes\n facts av   ace  table v  jost unavailable\n\u2018dels  ako publ ones  achive good results \n\n\u2018table v commonsese reasoning comparison\n\n  resus presented  table v  lear  gpt\nachieves best resus  hellasvag  davine003  best\n\u2018model  obqa isso good wo note  ress  obqa\nfre  reported forall ofthe modes  possibly davino03\njs ot  best mode achieving highest results  obqa\n\nnotall model report heir performance  al datasets \n\nox sect  hate ero fer shih perma\n able  syma wasnng ompaion\nes ord nove os shat cea ove ges\n xan ine nse gsm seh\n js rt spine  sp ak ed\nart  \u2018ite uae te aos ee"}, {"page_number": 34, "sentences": "\u2018table vil world knowledge comparison \u2018table ix arithmetic easing comparison\n\not    \n sone pcfiouse case models highy demanded bmhe es\n cain  cou ptaton apy ele vt shows es\n reals  diferent models  coding capability taree\ntable vie cot cpt compan rr\ntess tar large language models   cases ae hallucinating \n svc sings bruset rdction maces\nie hainan one ofthe important ors tm mesg\nsar tow much ang hnguage mc story  hae\nos \u2018meisuinghalsnaton ot ote hand eas ey\ngane ba \u2018scns oc nt   wien  eileen se \niee\u2014ts sorte sll thangs n wing male ihr det\nc5 ro ate fy pale llm   apie\nsore tse halal son   ducts  ns 0\nee eral nhs cd 203 vation   fe\n\nrithmic reasoning  anther challenging reasoning c\nbility  achieve gsmbk tor example contains rade school\n\u2018mathematical questions  respect othe answers table ix\n\u2018roves  insight  diferent model comparisons\n\netfrmed  another model judging  response  regal\nfo  actual answer 206 table x shows  evaluation \nableret models based   datasets\n\nvil challenges  future directions\n\n   scen   previous sections large language\n\u2018models  achieved impressive results inthe past 12 yeas"}, {"page_number": 35, "sentences": "\u2018table x hallucination evaluation\n\n   time thin 8 lla new  extemely active\nresearch area   pace  novation  neteasing eather\nhan lowing ether evolving  though \nsa sll numerous challenges ahead broly meaton\n ofthe challenges  main activ areas   know\n fa   worth noing  lem etllenges ae discussed\n details   work  kaddour etal 207 smaller   efficient language models\n\n\u201c   survey  large language models  \n\u2018ns hoe  nial push towards lager  eter\u201d  \ncleuly  sewarded  ever larger models like gpt\n\u2018tpeuing beter accuracy  performance  benchmarks\nhowever dhose large models ae cosy  inetcient \nseveral dimensions eg high latency response 10 isa cumentrescach trend  come   small\nlanguage models slms  costeffectve erative 10\nllm pariculsy  used  specific tasks hat might \nequi  full generality  larger models prominent works\ninthis direction include pa 208 phls 200  phi\n microsoft\n\n generally expect many esearch efor \n    fo tain stller   efficient models\n\u2018technigies   paranetereffiient haetuning peft\nteacherstident  cer forms  disiltion  se section\ntilt  conde tbe use  bald smaller model \n larger ones\n\n1b new postatenton architectural paradigms\n\n\u201ctransformer blacks  heen acral  constant pat \n  ument llm frameworks sr  abi ie ston ark\now mich longer  rcitectre wl  ne  \n   ext big architectural break inthe fed \ndeep learning  nlp since alexnet  2012   sen\n\u2018many architectures go     fasion incioding lstm gru soq2seg  transformers  een  dominant\npprach since  inception deere ear atlenion \nherman mechanism ding wransformers reel \n\u2018us  promising vesearch  allman approaches  ate\n\u2018 labeled ss stanton\n\n import cas   elas  postattention modes\nae   aed state space models ssm whi  notion\n stte space models   ong soy  machine lamin\n  ted   context  language models ssm\n\u2018 aaually used  reference tothe newer structure state space\n\u2018mode achtectre os  shot ee gu etl 29 \u2018cent models   category  mamba 30 hyena 210\nsd staped hyena 211 wile    models ate  competitive  teams \npesfomance  ladesbouds  efcency ako adress\nsn important challegge   traditional attention based\narchitectures dhe jack  support  larger content windows\n\n  good answer  many prompts rquises context\n example  response 1 recommend sme good movies\n  reiites  toc  context    well  \n\u2018movies ae avalible   ones    watched\ncootexe length  especially important  rag  lar\norion  fst might  ereved  injected nt  prompt\n generation se section tv\n\n\u2018 longer  context length   tokens  \nsqueeze imo  context mere informatio  model \nfatces 1  beer  response wll tong eootet  would  hard   mos\ntoremember everythings fice proces al  informt\nton aventonbased mols  highly nefiet  longer\n\u2018ontets  thats    expect  research \n\u201citfrent mechanisms  enable processing longer contexts\nland generally come    efficent architectures \u201c beings new architectures might   propose"}, {"page_number": 36, "sentences": "ltermatives forthe atenton mechanis  rater rethink \n\u2018whole transformer architecture early example  \n\u2018monarch miser 212 proposes  new architecture  uses\n\u2018  subquaatic primitive  achieves high hardware\n\u2018ficiency  gpus monarch matrices  along  sequence\ntength  move dimension \u2018   end ofthe specrum  worth mentioning\n\u2018hat    allenion compatible arcitestral mecha\nfms tht   cel gaining steam  proving \n\u2018alu  crating beter   powerful llms probably\nte best example  sich mechanism  mixture  experts\nmoe moes  ben around  machine leaning fr yeas\nven  dhe deep leaning era 213 sing popularity since   particularly   comext\n\u2018 transformer models  llm\n\n llm moes allow wo tain  extemely large model\n   patally instantiated daring inference\n\u2018shen    experts  turned  wherever  \ningheighting function  ow weight assigned  therm \nexample  glam model  12 tlion parameter bat\nluring inference  2  ofthe 6 expens  used 8\n\nmoes  nowadays  important component   \ncalled frontier llms e   advanced capable\n\u2018models gpt seit  mored  te based  3 moe\nsehteture     best performing llms  \nmixal 117 ae basically  moe version  preexisting\nlim\n\nfinally iis important  note  moes canbe used a8 2\n\u2018component   achtecture regardless  wheter itis based\n\u2018mftenton    fat mos  also heen applied \ns8albised llms lke mamba ctepioro2024moemarha comin 6 see moediven provements  fture\nregurdese ofthe underlying architect mulmodat models\n\nfurue llms  expected  multimodal  handle\n4 variety  datatypes text images  videos\nsudo   unified manner  opens poses \n\u2018 diverse sppictions  fields tike question answering\n\u2018content generation creative ate  hele robotics beyond already several prominent mulmdal\nelms  thee including llava 214 llava plus 215\ngpt l331 quenvt 116 nextgpt 316 bt  trend \nexpose oe contin evaaton ofthese models also  8\new research topic especialy conversational generative wsion\n\u2018models 217 mulitodal llms cap unlock huge ptenials\ninva varity  tsks    aleady heen 3 dence\ndrogrest inthis direction  needs 9 dedieated paper \n\u2018sca alls deta\n\n improved llm usoe  augmentation techniques\n\naas  described  setionv many   shortomings\n liniations  llm   hallucination   ae\n\u2018ressed  advanced prompt engineering use  oo\n\u2018rother augmentation techniques   expect  ony\n\u2018ontinied  accelerated reseirch   area hie worth\n\u2018mentioning  inthe specie case  software enginecrng works 218 tried yo automatically limuate  se\n  overall software engineering sro\n\nllmbased systems  aleady stating  replace \nchine teaming systems   unil recently using ther\nspproaches asa clear example ofthis llms   \n\u2018ployed  beter understand peopl preference apd interes\nland provide  personalized imerictions whether  cus\nomer servic content recommendation   pplictons\n\u2018 volves beter understanding  user preferences abd\nanalyzing ther past interactions  using  asthe comet \u2018  continue  sce research n  application  sage\n llms  nt  personalization amd recimmendatins\n\u2018 many  application seas rng  machine learning\ntechniques finally another important area  research  expect 10\n\u2018sther increased atenton    llmbaved agents nt\nimuliagent systems 172 173 174  development \ntem systems  acess  extemal tools  deision\n\u2018aking capabilities  bath exciting  challenging sc comin research  progress inthis importa tea hat\n\u2018ome argue cou leu  ariel general inligence agd\n\ne security  eticlresponsibe al\n\nensuring  robusness  security  llms \nadversarlatacks  oter vulnrabies  2 erica area\nfof reeach 219 llm ae ineeasingl deployed  rel\n\u2018word applications  need toe protected fom potential\nhres \u00a9 prevent   used 10 manipulate people \n\nadtessng ethical concems  bases  llm anor\nactive aeaof research effort ae eing made yo ence \nttlmg  fait unbiased capable  handling seastive\ninformation responsby  llms   used moce abd\n\u2018   tage number  people   dally bass making\nsire theyre nase  behave responsibly  eaca\n\nvin coxcwusion\n\n\u201c paper present  survey  llms developed  \npast fr yeas fist rovide  overview  eal pee\nined languge models eg 2s bert  review yee\npopular llm faies gpe llama palm  \nfepreseave lms survey methods  techniques\n\u2018 building augmenting nd using lum  review popular\nttlm datasts  benchmark  compare performance \n4 set  prominent models  pblic benchmarks final \nresent open challenges  future research dictions references\n\n1 2 rtn 5 21 1 hetman  boel  meme e bucns ta\n\ntring emt pale nage el\u201d st\n\n1 ed ab bel ston\n3 jee stal mets  speck epi mee pl\n\n15 ming st fal mt "}, {"page_number": 37, "sentences": "fy\n\nwa\n\nus\n\nua\n\nus\n\nus\n\nwi\n\nus\n\nww\n\nem\n\nen\n\nen\n\nes\n\neo\n\nes\n\n0\n\nen\n\nen\n\n en \nx te img cap mann cet  bbe\nire iopen vo 3p 22828 52 \n\nom\n\n\n\nom\n\nos\n\noo\n\nos\n\ncr\n\nro\nvo\nro\n\nwn\n\nwa\n\nro\n\nro\n\n\n\nwn\n\nro\n\n4 reber \u201cbarham chama c stim 5 girma\n\nesas 230\n\nable bem ste"}, {"page_number": 38, "sentences": "ry\n\ncy\n\n\u00ab\n\nca\n\nst\n\noo\n\nwn\n\nw\n\nws\n\nws\n\nws\n\nro\n\nwo\n\nst pg dregs sto\u201d abas aa\n\u2018momar pce itv 850\n\nine mp wndersanding  pst peeing 2018\nlance emer matinee\u201d open\n\u2018si ined tein erm phe haf acne\nsong xan tq j bi ad ty ln \u201cmase mkt\n lei lit gl hain md le \u2018tepceicn pep arc bi0 tsb 18\nangibsu san 6 soest ange mod\n\u2018  rl rin pat\nmch j woe jo q ya 0 pk\n\u20ac tive \u2018hin ros  oro browse\noe mx amb ac  \nsot insrmaon procrsne sem sa 8 pp 277302778\nopenal 2022 lading cage one aie pe\n\u2018penton\n\n\u201cnd tb hoshiotn \u201capc  song epee tn\niting mt set co fr reon fos \nep po 0 el 8\natm page ate kn lb\n1 bae ne  apr mot er sca och bg\nep\n\n\u2018aq fe syl  meh \u20ac raf ds ct\nboia meat lg sain \nb ar rem ri oe \u201cia se mien se\n\n56 pa tng xm gp cs lae\n\nisin centotensampsrn 328\n\n\n\nws\n\n0\n\nes\n\nvm\n\n\n\n\u20180\n\n\n\noa\n\nos\n\n6\n\ncy\n\n\n\n wang hl da j mel kat kr cat\ndm kbsiin sth ay ara io ass\n5 tema sires pak wo  mien\n\n ne art ste een aes nl\n\u2018taupe osoasunlsap ee\n\nine wt 01 cam omg\u201d wi pi ah 276 179\nsieur tiers ne"}, {"page_number": 39, "sentences": "vm\n\n\n\n\n\nvs\n\n\n\n\n\nion\n\nuo\n\n06\n\n0s\n\n06\n\ntion\n\ntio\n\nwr ot  thsi joie\nfes ing  tt \neruiae apm homie es\n5 han bong wie yh 8 sab sma \nsst rm das  hah\u201c  maly\n  ate gore amrepeite lagat\n\nwn\n\nwa\n\nws\n\nws\nws\n\nwn\n\nws\n\nim\n\nwn\n\nws\n\nton\n\n\n\nwo\n\nuo\n\n1\n\nostne aii pein ten se et  ermine ete \nsn abl nn hae\n\natric  en"}, {"page_number": 40, "sentences": "ss\n\n16\n\nun\n\n9\n\nsn\n\nuy\n\nfr\n\nus\n\nuu\n\nus\n\nus\n\n\n\nust\n\nist amat deep inf ming fm human pees\nearns en tan ee\n\n5s gaia ata beac 100\ncaen \u201cmadge reece seen cane\n\u2018act geet peed  es ao meng\nftc ncanon  comune nue h ketone\n1 \u2018touma liquors aso\nfee compt igen 201 pi tones\n\u2018sai helio ni\nie pecan  es nana rng ee \ngt bb x shee\n\u2018tty 32 tines  cling ard angus peds\nfare cone  get  ba\n\nuss\n\nsa\n\niss\n\nise\n\nus\n\nusm\n\nist\n\n\n\n\n\n\n\n0\n\nwn\n\nfn hope ste  begin tenon taman fe\n1 yee 2 hn ithe sgh mu ue kim  bm\n4b sr tegen te hol promt ee\nsup  cole vl a0  0 nine aa\n\u2018recmber 30  hp vt\nices mi eg arab wi poi\npitt tte bc\nsoin nt tel  md btn"}, {"page_number": 41, "sentences": "os\n\nisa\n\nsy\n\nise\n\nws\n\nuss\n\n\u2018scar tn 7p se tl\nava psaclamhologycngiqu 1026\n\nmj chi    zetbmoyer \u201ctio\nbse ia eee\none ava plang ep 6268\nseams cmr os 5 2019 one\n\nwn\n\nww\n\nws\n\nwa\n\nws\n\nwer\n\nwn\n\nws\n\nws\n\n0\n\neon\n\nomy\n\n1\n\noo\n\nco\n\noe\n\nem\n\nco\n\nop  tre cae\n3 alf enn ene catv  o8t\nonline avail tari cep 1n0 05487"}, {"page_number": 42, "sentences": "219 si \u00a5en wo 7\n\nmout\u201d decped one avie epietahco senate\n\nange tame  arse spot\n\n221 noa mion oleh ase pietahcom nvidia 225m nin one ase th somopenb mb fe fh nile bth cain\n\n228 pe yp oa\n\n20 eget te generon nore ole nae\nipancomiageacoh sno\n\n231 eeeten ingen onin abe sesion ae sapien oniae avie bapa\n\nte mp ey 231 tpn ose vase pietcombeno sania\n\noh techn oe aube ethos\n\n235 yah tpg one alle epsom sheba\n\n235 pace gstme onin aoahiie sepietahcom \u2018ene nim\n\n27 peas pometat onin arie sept co \u2018 ronpaas\n\n235 poeple pomp osi able sepia com lhe\n29 feeb te onn alb sept co\n\n2m eo onl aoi heo\n\n24 pam qa one ava pstmt\n\n20 erate wei ale arie mptnncomneie 2 ie ns mink nna sapo\n\n1 open source toolkits  llm development \ndeployment\n\n\u2018  various frameworks  ibraies developed \nllm training evaluation  deployment ad covering every\nsingle framewerk   ofthis pipers scope fy 1\nprovide rit ntodnction   ofthe  propel oes\nrouped  diferent categories llm truningnference frameworks\n\n ofthe popular famewovks whic ae wsful  llm\nteaming includes note ht    canbe used beyond\nilm taining \n\ndeepspeed 220  4 deep leaming optimization lbary\n\u2018ar makes disubuted uaiing  inference ass elie\n effective deepspeed enables wodd  power lane\nftuage modsis like mts30b  bloom eusy\nforte deep ieaming optimization software suite  powers\nluprecedeted scale  speed  bh ning  inference\n deepspeed  \n\n\u2018transformers 221  hibary  huggingface \n\u2018roves thousands  peetrained models  perform tasks \naierent modalities   text vision  audio using\nietuined models one  edie compte cont carbon\nfooipint  save  time  resoureesrequted  tain\nmodel fon sath\n\nmegatronlm 222   lage powerful transformer\ndeveloped   applied deep learning research team\nsunvidia contains efficients mode pall nso 32\nquence  pipe mulnode preaning  wans\nfonmer base dee suchas gpt bert  ts using tinal\ncision\n\nbmtrain 223 ian ficient large model taining wok\n\u2018 cane used  tain large models  tens  bilo \nparameters  win mes  dsiated manner \nespn  code  simple  stanalone taining gptneox 224 leverages many ofthe  features \ntechnologies   popular mepaton deepspeed teary bat\n\u2018sith subtandalyieresed ust  novel optimizations\n\nlora 225 library provides  suppor  lowrank\nadaptation  large language models 1 redoes  number\n\u2018 trinale parameters hy learning pats  rink decomposton\nimatces  eezing  original weighs  vasly"}, {"page_number": 43, "sentences": "seduces  storage eequtement  lage language models\n\u2018mlped  specific tusks std enables efficient tskswtching\n\u2018 deploynent  without ingeducing inference latency tlora also outperforms several  adaption methods ia\ncluding adapter pretiuning  fnetuning\n\ncolosalal tary 226 provides  coleton  parle\ncomponents aims 10 support developers  write \n\u2018ustrbuted dee learing models  ike ow hey wee \n\u2018model  thet laptop provide wserfienly tols 19\nlst distibted wining  inference tna  lies terme  paalelsm strates  suppor data pale\npipeline paalelism sequence parallelism zero redundancy\noptimizer zero 140  autoparleism\n\nb deployment toots\n\n provide  overview   ofthe  popular llm\nployment tools \n\nfastchat 227   open platform  training sev\ning  evaliting large language model hised chats\nfiat core features ince  timing  evaluation\nce  state models eg vieun mtfbench \n2 dhnbated multcmodel serving sem ith web ul \n\u2018opens compatible restio apl shypilot 25  \u00abframework  raning llms al\n batch jobs   cloud offering maximum cos snes\niighest gpu seiabity  managed execution\n\nllm 229 sa fast  easyuse libary  llm \nference nd serving vllm ssemiessly supports many hlgging\nace models nctoding  following architectures aquila\nbrichuan bloom chaglm decilm falcon gpt big\nce leama leama 2 mistal mista mpt opt quen\nyivand many textgeneration inference 230   ook  deploying\n serving large language models llms tcl enables\nhigheefomance text generation    popular open\nsource llms inclading llama falcon srcove bloom gpencox  moe\n\nlangchain 231  framework  developing apis\nsions poered  language models tt enables applications \n\n ate comextawate comect 4 language model 10\nsources  coment romp istations  shoe\naimpls content ground st response  et reason rely  2 language model  reston \n  answer based om provided context  30\n  ake ete\n\n\u2018openllm 232 isan opensource platform designed 19\nfact te deployment  operation  args language \ncls llm  realeworkd applications  openllm \n run inference   opensouree llm deploy  \n\u2018 cloud  oorprenies  build powerful al applications\n\nembedchain 233   open source rag framework\n makes  easy  create  deploy  apps emedchain\nstreamlines  eration  rag applications ofering  seam\ntess process  managing varie types  unstrctred dt\nteeftctenty segments data nto manageale chunks generates\n\nselevant embeddings  stores    vector database \n\u2018puiized eal\n\nautogen 234  ffamevowk  enables  devel\n\u2018opment  lem applications using multiple agents  \nconverse     solve tusks autogen agente\nse customizable comersable  seamlessly allow human\nparcpaion   operate  various modes  employ\n\u2018combinations  llms human inputs apd tools\n\nabyagh 285 isan autonomous arif otelgence\nagent   designed 0 generate  execute asks based \nieven objectives himeses ctingedgelechnoogis fom penal pinecone langchain  chroma  automate asks\nfant achieve specific goals blog pos dive\nio  unigoe features  babyagt  explore   \n\u2018reaming task automation\n\n\u00a9 prompting libraries\n\nguidance 236   programming paradigm tht offers\nsuperior contol  effiency compared  conventional\nprompting  chaining allows users  eonstan generation\n2  regex  cfgs  well   inerleave conta\nondtional oops  generation seamlessly\n\nprompttools 237 offers  set  opensource self\nhose tools fr experimenting wih testing evaluating\ntelm vector diabases prompts core idea  10\nsable developers  evaluate sng fair traces ike\n\u2018rie notebooks  local playground\n\npromptbench 2 isa pyorcbased python package fr\nvaluation  large languige models llms 1 provides\nseerdriendly apis  researchers yo condost evaluation om\nilm\n\nprompt 23  tool fr testing  evlting llm\n\u2018output quality systematically test prompts models \nfrag  predefined test cases\n\nvectordb\n\nfeiss 259  library developed  facebook ai \nsearch  provides ecen smilnity search  clstering\nfof dense vectors tt  designed  use wih largescale highaimensional data  suppons several index types \nlgrthms  various se cases\n\nmitvas 240   opensource vector database built 10\npower embedding similanty search   applications mil\n\u2018foe maker tnsrictred data serch  aevessibe  pro\n\u2018ide consistent ser experience regardless ofthe deployment\n\nqdrant 241   vector similarity search engine ad\n\u2018vector database provides productioneady service  2\nconvenient api  soe search  manage points\u2014sectors\n\u2018  ational payioad qarant  talred  extended\nhitering support environment\n\nweavate 22   opensource graphqlbused vee\ntor scare engine tht enables simiaity search  bigh\nmensional data itis open soure commercial ee\n\u2018om offers ado fstures support ard managed services\n\n    popular options inches llamalndes\n248  pinecone"}]